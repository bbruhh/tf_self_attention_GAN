{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vvAbX3jDmhwp"
   },
   "source": [
    "## 1 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H8rTvhy1f2-k"
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "import os\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LFW9vyapfrpJ"
   },
   "source": [
    "#### set up local working directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1266,
     "status": "ok",
     "timestamp": 1541544618146,
     "user": {
      "displayName": "Jake Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/-qpuoEuHtM6k/AAAAAAAAAAI/AAAAAAAAASM/Yutboujk5B4/s64/photo.jpg",
      "userId": "09150888735248367162"
     },
     "user_tz": 300
    },
    "id": "zU8EPXVAfX-g",
    "outputId": "db00f712-07bd-4b26-f7cf-5d6e7ed62fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created  /content/SAGAN/dataset\n",
      "created  /content/SAGAN/trained_models\n"
     ]
    }
   ],
   "source": [
    "# get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# choose a local (colab) directory to store the data.\n",
    "local_root_dir = os.path.expanduser('{}/SAGAN/'.format(cwd))\n",
    "try:\n",
    "  # make root directory\n",
    "  os.makedirs(local_root_dir)\n",
    "  # create sub directories\n",
    "  for subdir in ['dataset', 'trained_models']:\n",
    "    os.makedirs(os.path.join(local_root_dir, subdir))\n",
    "    print('created ',os.path.join(local_root_dir, subdir))\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrba1lQ1fewq"
   },
   "source": [
    "#### download dataset hosted in G-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U2ZRdo370ITw"
   },
   "outputs": [],
   "source": [
    "# 1. Authenticate and create the PyDrive client.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3014,
     "status": "ok",
     "timestamp": 1541544649165,
     "user": {
      "displayName": "Jake Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/-qpuoEuHtM6k/AAAAAAAAAAI/AAAAAAAAASM/Yutboujk5B4/s64/photo.jpg",
      "userId": "09150888735248367162"
     },
     "user_tz": 300
    },
    "id": "Dlgyxm9uBR5x",
    "outputId": "06692dd4-5555-4274-9d3f-d0649826dbae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celeba_dataset.tfrecord 1Xs7JtH1ChBVu0uWBYGOnBYaqbbtierH_\n",
      "celeba_dataset_highres.tfrecord 127qyHBbUxXdkDcPa92TgYI4e5zk63EHR\n"
     ]
    }
   ],
   "source": [
    "# G-Drive shareId of the dataset folder\n",
    "fileId = '1Pn8C73WWQ6tLGvfg2_GGQwIXczKj9td4'\n",
    "file_list = drive.ListFile(\n",
    "    {'q': \"'{}' in parents\".format('1Pn8C73WWQ6tLGvfg2_GGQwIXczKj9td4')}).GetList()\n",
    "for f in file_list:\n",
    "  print(f['title'], f['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MJvnmCfSBZkA"
   },
   "outputs": [],
   "source": [
    "# download from G-Drive to local dataset directory\n",
    "fname = os.path.join(local_root_dir+'dataset/', file_list[0]['title'])\n",
    "f_ = drive.CreateFile({'id': file_list[0]['id']})\n",
    "f_.GetContentFile(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2859,
     "status": "ok",
     "timestamp": 1541544693308,
     "user": {
      "displayName": "Jake Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/-qpuoEuHtM6k/AAAAAAAAAAI/AAAAAAAAASM/Yutboujk5B4/s64/photo.jpg",
      "userId": "09150888735248367162"
     },
     "user_tz": 300
    },
    "id": "aPFSMuP-GVgP",
    "outputId": "42711348-3219-4724-f9a5-18c615c724a1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celeba_dataset.tfrecord\n"
     ]
    }
   ],
   "source": [
    "# check tfrecord was downloaded correctly\n",
    "!ls /content/SAGAN/dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 29508,
     "status": "ok",
     "timestamp": 1541544725316,
     "user": {
      "displayName": "Jake Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/-qpuoEuHtM6k/AAAAAAAAAAI/AAAAAAAAASM/Yutboujk5B4/s64/photo.jpg",
      "userId": "09150888735248367162"
     },
     "user_tz": 300
    },
    "id": "EmZ-w6er6HUA",
    "outputId": "e12789e5-0109-4d0f-bf54-35c6c8f6d771"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# mount G-drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e7Ei7OTPesOF"
   },
   "source": [
    "## 2 Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YukcF0QCerPp"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model import tag_constants, signature_constants\n",
    "from tensorflow.python.saved_model.signature_def_utils_impl import predict_signature_def\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vj5gX4Wj9-ur"
   },
   "source": [
    "## 3 Input Data Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BLucSBQsq43s"
   },
   "source": [
    "#### parse tf example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "du3tJiioedsb"
   },
   "outputs": [],
   "source": [
    "def parse_img_example(record, target_height=128, target_width=128):\n",
    "    \"\"\"\n",
    "    function to parse tfRecord examples back into tensors.\n",
    "    \"\"\"\n",
    "    keys_to_features = {\n",
    "        \"image\" : tf.FixedLenFeature((), tf.string),\n",
    "        \"height\": tf.FixedLenFeature((), tf.int64),\n",
    "        \"width\" : tf.FixedLenFeature((), tf.int64)\n",
    "    }    \n",
    "    features = tf.parse_single_example(record, keys_to_features)\n",
    "    # convert features to tensors\n",
    "    image = tf.decode_raw(features['image'], tf.uint8)\n",
    "    height = tf.cast(features['height'], tf.int64)\n",
    "    width = tf.cast(features['width'], tf.int64)\n",
    "    # reshape input to original dimensions and cast image to type float\n",
    "    image = tf.reshape(image, (height, width, 3))\n",
    "    # reshape images via center crop and pad to same shape\n",
    "    image = tf.image.resize_image_with_crop_or_pad(image, target_height, target_width)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqX0DyR9gT-v"
   },
   "source": [
    "#### normalize image tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9CqkV1R7gZsU"
   },
   "outputs": [],
   "source": [
    "def normalizer(image, dtype):\n",
    "    # normalize image pixel values to within [-1,1]\n",
    "    image = tf.cast(image, dtype=dtype) / 128.0 - 1.0\n",
    "    # noise addition normalization\n",
    "    image += tf.random_uniform(shape=tf.shape(image), minval=0., maxval=1./128., dtype=dtype)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kRdryHKZgb8z"
   },
   "source": [
    "#### construct tf dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zPmW5TheggFe"
   },
   "outputs": [],
   "source": [
    "def create_tfdataset(tfrecord_file, img_height, img_width, shuffle_buffer, epochs, batch_size, pThreads=4):\n",
    "    # create a tf dataset obj from TFRecord file\n",
    "    dataset = tf.data.TFRecordDataset(tfrecord_file)\n",
    "    # use dataset.map() in conjunction with the parse_exmp function to \n",
    "    # de-serialize each example record in TFRecord file\n",
    "    dataset = dataset.map(lambda img_exmpl: parse_img_example(img_exmpl, img_height, img_width), num_parallel_calls=pThreads)\n",
    "    # normalize image\n",
    "    dataset = dataset.map(lambda image: normalizer(image, dtype=tf.float32), num_parallel_calls=pThreads)\n",
    "    # configure dataset epoch, shuffle, padding and batching operations\n",
    "    dataset = dataset.shuffle(shuffle_buffer).repeat(epochs).batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T8rQ7lX8gp2b"
   },
   "source": [
    "## 4 Generator & Discriminator Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QUQm8Pvz9AE0"
   },
   "source": [
    "#### power iteration function for applying spectral normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4EkZQ2S4cSAG"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras.initializers as initializers\n",
    "import tensorflow.keras.constraints as constraints\n",
    "import tensorflow.keras.activations as activations\n",
    "import tensorflow.keras.regularizers as regularizers\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.ops import standard_ops\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.layers import utils\n",
    "from tensorflow.python.ops import nn\n",
    "from tensorflow.python.layers import core\n",
    "from tensorflow.python.layers import convolutional as tf_convolutional_layers\n",
    "from tensorflow.python.util.tf_export import tf_export\n",
    "\n",
    "\n",
    "def _l2normalizer(v, epsilon=1e-12):\n",
    "    return v / (K.sum(v ** 2) ** 0.5 + epsilon)\n",
    "\n",
    "\n",
    "def power_iteration(W, u, rounds=1):\n",
    "    '''\n",
    "    Accroding the paper, we only need to do power iteration one time.\n",
    "    '''\n",
    "    _u = u\n",
    "\n",
    "    for i in range(rounds):\n",
    "        _v = _l2normalizer(K.dot(_u, W))\n",
    "        _u = _l2normalizer(K.dot(_v, K.transpose(W)))\n",
    "\n",
    "    W_sn = K.sum(K.dot(_u, W) * _v)\n",
    "    return W_sn, _u, _v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VB58nrv39Hcv"
   },
   "source": [
    "#### custom dense layer with spectral norm applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GJ1ceV0hTlK"
   },
   "outputs": [],
   "source": [
    "# dense layer with spectral norm toggle\n",
    "@tf_export('keras.layers.Dense')\n",
    "class Dense(core.Dense):\n",
    "    def __init__(self, \n",
    "                 units,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 spectral_normalization=True,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # initializing parent class\n",
    "        super(Dense, self).__init__(\n",
    "            units=int(units), \n",
    "            activation=activations.get(activation),\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=initializers.get(kernel_initializer),\n",
    "            bias_initializer=initializers.get(bias_initializer),\n",
    "            kernel_regularizer=regularizers.get(kernel_regularizer),\n",
    "            bias_regularizer=regularizers.get(bias_regularizer),\n",
    "            activity_regularizer=regularizers.get(activity_regularizer),\n",
    "            kernel_constraint=constraints.get(kernel_constraint),\n",
    "            bias_constraint=constraints.get(bias_constraint),\n",
    "            **kwargs)\n",
    "\n",
    "        self.u = K.random_normal_variable([1, units], 0, 1, dtype=self.dtype, name=\"sn_estimate\")  # [1, out_channels]\n",
    "        self.spectral_normalization = spectral_normalization\n",
    "\n",
    "    def compute_spectral_normal(self, training=True):\n",
    "        # Spectrally Normalized Weight\n",
    "        if self.spectral_normalization:\n",
    "            # Get kernel tensor shape [batch, units]\n",
    "            W_shape = self.kernel.shape.as_list()\n",
    "\n",
    "            # Flatten the Tensor\n",
    "            W_mat = K.reshape(self.kernel, [W_shape[-1], -1])  # [out_channels, N]\n",
    "\n",
    "            W_sn, u, v = power_iteration(W_mat, self.u)\n",
    "\n",
    "            if training:\n",
    "                # Update estimated 1st singular vector\n",
    "                self.u.assign(u)\n",
    "\n",
    "            return self.kernel / W_sn\n",
    "        else:\n",
    "            return self.kernel\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        inputs = ops.convert_to_tensor(inputs, dtype=self.dtype)\n",
    "        shape = inputs.get_shape().as_list()\n",
    "        if len(shape) > 2:\n",
    "            # Broadcasting is required for the inputs.\n",
    "            outputs = standard_ops.tensordot(inputs, self.compute_spectral_normal(training), [[len(shape) - 1],\n",
    "                                                                   [0]])\n",
    "            # Reshape the output back to the original ndim of the input.\n",
    "            if not context.executing_eagerly():\n",
    "                output_shape = shape[:-1] + [self.units]\n",
    "                outputs.set_shape(output_shape)\n",
    "        else:\n",
    "            outputs = gen_math_ops.mat_mul(inputs, self.kernel)\n",
    "        if self.use_bias:\n",
    "            outputs = nn.bias_add(outputs, self.bias)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)  # pylint: disable=not-callable\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7FCpci2g9OYz"
   },
   "source": [
    "#### custom convolution-2D function with spectral norm applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rkjs-hRbhXRp"
   },
   "outputs": [],
   "source": [
    "# dense layer with spectral norm toggle\n",
    "@tf_export('keras.layers.Conv2D', 'keras.layers.Convolution2D')\n",
    "class Conv2D(tf_convolutional_layers.Conv2D, Layer):\n",
    "      \n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 dilation_rate=(1, 1),\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 spectral_normalization=True,\n",
    "                 bias_constraint=None,\n",
    "                 **kwargs):\n",
    "      \n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "            \n",
    "        super(Conv2D, self).__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            dilation_rate=dilation_rate,\n",
    "            activation=activations.get(activation),\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=initializers.get(kernel_initializer),\n",
    "            bias_initializer=initializers.get(bias_initializer),\n",
    "            kernel_regularizer=regularizers.get(kernel_regularizer),\n",
    "            bias_regularizer=regularizers.get(bias_regularizer),\n",
    "            activity_regularizer=regularizers.get(activity_regularizer),\n",
    "            kernel_constraint=constraints.get(kernel_constraint),\n",
    "            bias_constraint=constraints.get(bias_constraint),\n",
    "            **kwargs)\n",
    "\n",
    "        self.u = K.random_normal_variable([1, filters], 0, 1, dtype=self.dtype, name=\"sn_estimate\")  # [1, out_channels]\n",
    "        self.spectral_normalization = spectral_normalization\n",
    "\n",
    "    def compute_spectral_normal(self, training=True):\n",
    "        # Spectrally Normalized Weight\n",
    "        if self.spectral_normalization:\n",
    "            # Get kernel tensor shape [kernel_h, kernel_w, in_channels, out_channels]\n",
    "            W_shape = self.kernel.shape.as_list()\n",
    "\n",
    "            # Flatten the Tensor\n",
    "            W_mat = K.reshape(self.kernel, [W_shape[-1], -1])  # [out_channels, N]\n",
    "\n",
    "            W_sn, u, v = power_iteration(W_mat, self.u)\n",
    "\n",
    "            if training:\n",
    "                # Update estimated 1st singular vector\n",
    "                self.u.assign(u)\n",
    "\n",
    "            return self.kernel / W_sn\n",
    "        else:\n",
    "            return self.kernel\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "\n",
    "        outputs = K.conv2d(\n",
    "            inputs,\n",
    "            self.compute_spectral_normal(training),\n",
    "            strides=self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format,\n",
    "            dilation_rate=self.dilation_rate)\n",
    "\n",
    "        if self.bias is not None:\n",
    "            outputs = K.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=self.data_format)\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'data_format': self.data_format,\n",
    "            'dilation_rate': self.dilation_rate,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint)\n",
    "        }\n",
    "        base_config = super(Conv2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "19px2aPkYCKB"
   },
   "source": [
    "#### custom convolution-2D transpose function with spectral norm applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GBPv5Mj2YX1k"
   },
   "outputs": [],
   "source": [
    "# dense layer with spectral norm toggle\n",
    "@tf_export('tf.keras.layers.Conv2DTranspose',\n",
    "           'tf.keras.layers.Convolution2DTranspose')\n",
    "class Conv2DTranspose(tf_convolutional_layers.Conv2DTranspose, Layer):\n",
    "  \n",
    "    def __init__(self,\n",
    "                 filters,\n",
    "                 kernel_size,\n",
    "                 strides=(1, 1),\n",
    "                 padding='valid',\n",
    "                 data_format=None,\n",
    "                 activation=None,\n",
    "                 use_bias=True,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 bias_initializer='zeros',\n",
    "                 kernel_regularizer=None,\n",
    "                 bias_regularizer=None,\n",
    "                 activity_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 bias_constraint=None,\n",
    "                 spectral_normalization=True,\n",
    "                 **kwargs):\n",
    "      \n",
    "        if data_format is None:\n",
    "            data_format = K.image_data_format()\n",
    "            \n",
    "        super(Conv2DTranspose, self).__init__(\n",
    "            filters=filters,\n",
    "            kernel_size=kernel_size,\n",
    "            strides=strides,\n",
    "            padding=padding,\n",
    "            data_format=data_format,\n",
    "            activation=activations.get(activation),\n",
    "            use_bias=use_bias,\n",
    "            kernel_initializer=initializers.get(kernel_initializer),\n",
    "            bias_initializer=initializers.get(bias_initializer),\n",
    "            kernel_regularizer=regularizers.get(kernel_regularizer),\n",
    "            bias_regularizer=regularizers.get(bias_regularizer),\n",
    "            activity_regularizer=regularizers.get(activity_regularizer),\n",
    "            kernel_constraint=constraints.get(kernel_constraint),\n",
    "            bias_constraint=constraints.get(bias_constraint),\n",
    "            **kwargs)\n",
    "        \n",
    "        self.spectral_normalization = spectral_normalization\n",
    "        self.u = K.random_normal_variable([1, filters], 0, 1, dtype=self.dtype, name=\"sn_estimate\")  # [1, out_channels]\n",
    "\n",
    "    def compute_spectral_normal(self, training=True):\n",
    "        # Spectrally Normalized Weight\n",
    "\n",
    "        if self.spectral_normalization:\n",
    "            # Get the kernel tensor shape\n",
    "            W_shape = self.kernel.shape.as_list()\n",
    "\n",
    "            # Flatten the Tensor\n",
    "            # For transpose conv, the kernel shape is [H,W,Out,In]\n",
    "            W_mat = K.reshape(self.kernel, [W_shape[-2], -1])  # [out_c, N]\n",
    "\n",
    "            sigma, u, v = power_iteration(W_mat, self.u)\n",
    "\n",
    "            if training:\n",
    "                # Update estimated 1st singular vector\n",
    "                self.u.assign(u)\n",
    "\n",
    "            return self.kernel / sigma\n",
    "        else:\n",
    "            return self.kernel\n",
    "\n",
    "    # Overwrite the call() method to include Spectral normalization call\n",
    "    def call(self, inputs, training=True):\n",
    "        inputs_shape = array_ops.shape(inputs)\n",
    "        batch_size = inputs_shape[0]\n",
    "        if self.data_format == 'channels_first':\n",
    "            c_axis, h_axis, w_axis = 1, 2, 3\n",
    "        else:\n",
    "            c_axis, h_axis, w_axis = 3, 1, 2\n",
    "\n",
    "        height, width = inputs_shape[h_axis], inputs_shape[w_axis]\n",
    "        kernel_h, kernel_w = self.kernel_size\n",
    "        stride_h, stride_w = self.strides\n",
    "\n",
    "        # Infer the dynamic output shape:\n",
    "        out_height = utils.deconv_output_length(height,\n",
    "                                                kernel_h,\n",
    "                                                self.padding,\n",
    "                                                stride_h)\n",
    "        out_width = utils.deconv_output_length(width,\n",
    "                                               kernel_w,\n",
    "                                               self.padding,\n",
    "                                               stride_w)\n",
    "        if self.data_format == 'channels_first':\n",
    "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
    "            strides = (1, 1, stride_h, stride_w)\n",
    "        else:\n",
    "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
    "            strides = (1, stride_h, stride_w, 1)\n",
    "\n",
    "        output_shape_tensor = array_ops.stack(output_shape)\n",
    "        outputs = nn.conv2d_transpose(\n",
    "            inputs,\n",
    "            self.compute_spectral_normal(training=training),\n",
    "            output_shape_tensor,\n",
    "            strides,\n",
    "            padding=self.padding.upper(),\n",
    "            data_format=utils.convert_data_format(self.data_format, ndim=4))\n",
    "\n",
    "        if not context.executing_eagerly():\n",
    "            # Infer the static output shape:\n",
    "            out_shape = inputs.get_shape().as_list()\n",
    "            out_shape[c_axis] = self.filters\n",
    "            out_shape[h_axis] = utils.deconv_output_length(out_shape[h_axis],\n",
    "                                                           kernel_h,\n",
    "                                                           self.padding,\n",
    "                                                           stride_h)\n",
    "            out_shape[w_axis] = utils.deconv_output_length(out_shape[w_axis],\n",
    "                                                           kernel_w,\n",
    "                                                           self.padding,\n",
    "                                                           stride_w)\n",
    "            outputs.set_shape(out_shape)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = nn.bias_add(\n",
    "                outputs,\n",
    "                self.bias,\n",
    "                data_format=utils.convert_data_format(self.data_format, ndim=4))\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            'filters': self.filters,\n",
    "            'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides,\n",
    "            'padding': self.padding,\n",
    "            'data_format': self.data_format,\n",
    "            'activation': activations.serialize(self.activation),\n",
    "            'use_bias': self.use_bias,\n",
    "            'kernel_initializer': initializers.serialize(self.kernel_initializer),\n",
    "            'bias_initializer': initializers.serialize(self.bias_initializer),\n",
    "            'kernel_regularizer': regularizers.serialize(self.kernel_regularizer),\n",
    "            'bias_regularizer': regularizers.serialize(self.bias_regularizer),\n",
    "            'activity_regularizer':\n",
    "                regularizers.serialize(self.activity_regularizer),\n",
    "            'kernel_constraint': constraints.serialize(self.kernel_constraint),\n",
    "            'bias_constraint': constraints.serialize(self.bias_constraint),\n",
    "            'spectral_normalization': self.spectral_normalization\n",
    "        }\n",
    "        base_config = super(Conv2DTranspose, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9j214tOhnIpn"
   },
   "source": [
    "#### self-attention layer definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4tAecZu8nIJP"
   },
   "outputs": [],
   "source": [
    "def hw_flatten(x):\n",
    "    # Input shape x: [BATCH, HEIGHT, WIDTH, CHANNELS]\n",
    "    # flat the feature volume across the tensor width and height\n",
    "    x_shape = tf.shape(x)\n",
    "    return tf.reshape(x, [x_shape[0], -1, x_shape[-1]]) # return [BATCH, W*H, CHANNELS]\n",
    "  \n",
    "\n",
    "class SelfAttention(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self, number_of_filters, dtype):\n",
    "        super(SelfAttention, self).__init__()\n",
    "\n",
    "        self.f = Conv2D(filters=number_of_filters//8, \n",
    "                        kernel_size=1, \n",
    "                        strides=1,\n",
    "                        spectral_normalization=True,                 \n",
    "                        padding='SAME', \n",
    "                        name='f_x',\n",
    "                        activation=None, \n",
    "                        dtype=dtype)\n",
    "\n",
    "        self.g = Conv2D(filters=number_of_filters//8, \n",
    "                        kernel_size=1, \n",
    "                        strides=1,\n",
    "                        spectral_normalization=True,                 \n",
    "                        padding='SAME', \n",
    "                        name='g_x',\n",
    "                        activation=None, \n",
    "                        dtype=dtype)\n",
    "\n",
    "        self.h = Conv2D(filters=number_of_filters, \n",
    "                        kernel_size=1, \n",
    "                        strides=1,\n",
    "                        spectral_normalization=True,                 \n",
    "                        padding='SAME',\n",
    "                        name='h_x',\n",
    "                        activation=None, \n",
    "                        dtype=dtype)\n",
    "\n",
    "        self.gamma = tfe.Variable(0., dtype=dtype, trainable=True, name=\"gamma\")\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "\n",
    "        \n",
    "    def call(self, x):\n",
    "        \n",
    "        f = self.f(x)\n",
    "        g = self.g(x)\n",
    "        h = self.h(x)\n",
    "\n",
    "        f_flatten = hw_flatten(f)\n",
    "        g_flatten = hw_flatten(g)\n",
    "        h_flatten = hw_flatten(h)\n",
    "        \n",
    "        s = tf.matmul(a=g_flatten, \n",
    "                      b=f_flatten, \n",
    "                      transpose_b=True) # [B,N,C] * [B, N, C] = [B, N, N]\n",
    "\n",
    "        b = tf.nn.softmax(s, axis=-1)\n",
    "        o = tf.matmul(b, h_flatten)\n",
    "        y = self.gamma * tf.reshape(o, tf.shape(x)) + x\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EnbgN15h1kN"
   },
   "source": [
    "#### discriminator model class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EYXjVP9ly0EV"
   },
   "outputs": [],
   "source": [
    "APPLY_SPECTRAL_NORM=True\n",
    "APPLY_SELF_ATTENTION=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuaMRzUwhHGR"
   },
   "outputs": [],
   "source": [
    "class Discriminator(tf.keras.Model):    \n",
    "    \"\"\"\n",
    "    Create the discriminator network\n",
    "    :param images: Tensor of input image(s)\n",
    "    :param alpha: Scalar value specifying the degree of leakage in leaky relu\n",
    "    :param reuse: Boolean if the weights should be reused\n",
    "    :return: Tuple of (tensor output of the discriminator, tensor logits of the discriminator)\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha, dtype):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        # block 1\n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=4, strides=2, padding='same', \n",
    "                            data_format='channels_last', use_bias=True, \n",
    "                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                            activation=None, name='d_conv1')\n",
    "        self.conv2 = Conv2D(filters=64, kernel_size=4, strides=2, padding='same', \n",
    "                            data_format='channels_last', use_bias=True, \n",
    "                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                            activation=None, name='d_conv2')\n",
    "        \n",
    "        self.attention = SelfAttention(64, dtype=dtype)\n",
    "        \n",
    "        self.conv3 = Conv2D(filters=128, kernel_size=4, strides=2, padding='same', \n",
    "                            data_format='channels_last', use_bias=True, \n",
    "                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                            activation=None, name='d_conv3')\n",
    "        self.conv4 = Conv2D(filters=256, kernel_size=4, strides=2, padding='same', \n",
    "                            data_format='channels_last', use_bias=True, \n",
    "                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                            activation=None, name='d_conv4')\n",
    "        self.conv5 = Conv2D(filters=512, kernel_size=4, strides=2, padding='same', \n",
    "                            data_format='channels_last', use_bias=True,\n",
    "                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                            activation=None, name='d_conv5')\n",
    "        self.flat = tf.keras.layers.Flatten()\n",
    "        self.fc1 = Dense(units=1, dtype=dtype, \n",
    "                         spectral_normalization=APPLY_SPECTRAL_NORM, \n",
    "                         activation=None, name='d_logits')\n",
    "\n",
    "\n",
    "    def call(self, inputs, is_training):\n",
    "        # block 1 operation\n",
    "        net = self.conv1(inputs, training=is_training)\n",
    "        net = tf.nn.leaky_relu(net, alpha=self.alpha)\n",
    "        # block 2 operation\n",
    "        net = self.conv2(net, training=is_training)\n",
    "        net = tf.nn.leaky_relu(net, alpha=self.alpha)\n",
    "        \n",
    "        if APPLY_SELF_ATTENTION:\n",
    "            # self attention operation\n",
    "            net = self.attention(net)\n",
    "        \n",
    "        # block 3 operation\n",
    "        net = self.conv3(net, training=is_training)\n",
    "        net = tf.nn.leaky_relu(net, alpha=self.alpha)\n",
    "        # block 4 operation\n",
    "        net = self.conv4(net, training=is_training)\n",
    "        net = tf.nn.leaky_relu(net, alpha=self.alpha)\n",
    "#         # block 5 operation\n",
    "#         net = self.conv5(net)\n",
    "#         net = tf.nn.leaky_relu(net, alpha=self.alpha)\n",
    "        # logit output\n",
    "        net = self.flat(net)\n",
    "        logits = self.fc1(net)\n",
    "\n",
    "        return logits\n",
    "    \n",
    "\n",
    "    def compute_loss(self, d_logits_real, d_logits_fake):\n",
    "#         loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#                                    logits=d_logits_real, \n",
    "#                                    labels=tf.ones_like(d_logits_real)))\n",
    "#         loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#                                    logits=d_logits_fake, \n",
    "#                                    labels=tf.zeros_like(d_logits_fake)))\n",
    "#         return loss_real + loss_fake\n",
    "\n",
    "        # Hinge loss\n",
    "        real_loss = tf.reduce_mean(tf.nn.relu(1. - d_logits_real))\n",
    "        fake_loss = tf.reduce_mean(tf.nn.relu(1. + d_logits_fake))\n",
    "\n",
    "        return real_loss + fake_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8kR93QNehRwk"
   },
   "source": [
    "#### generator model class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hI6dJfGFhEfy"
   },
   "outputs": [],
   "source": [
    "class Generator(tf.keras.Model):\n",
    "    \"\"\"\n",
    "    Create the generator network\n",
    "    :param z: Input z\n",
    "    :param is_train: Boolean if generator is being used for training\n",
    "    :return: The tensor output of the generator\n",
    "    \"\"\"\n",
    "    def __init__(self, dtype):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # init all layer components; note no actual computation is done\n",
    "        # fully connected layer 1\n",
    "        self.fc1 = Dense(units=4*4*512, dtype=dtype, activation='relu', \n",
    "                         spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                         name='g_fc1')\n",
    "        # conv transpose + batch norm layer 1\n",
    "        self.transp_conv1 = Conv2DTranspose(filters=512, kernel_size=4, strides=2, \n",
    "                                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                                            padding='same', activation=None, name='g_tr_conv1')\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization(scale=False, dtype=dtype, fused=False, name='g_bn1')\n",
    "        # conv transpose + batch norm layer 2\n",
    "        self.transp_conv2 = Conv2DTranspose(filters=128, kernel_size=4, strides=2,\n",
    "                                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                                            padding='same', activation=None, name='g_tr_conv2')\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization(scale=False, dtype=dtype, fused=False, name='g_bn2')\n",
    "        # conv transpose + batch norm layer 3\n",
    "        self.transp_conv3 = Conv2DTranspose(filters=64, kernel_size=4, strides=2,\n",
    "                                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                                            padding='same', activation=None, name='g_tr_conv3')\n",
    "        self.bn3 = tf.keras.layers.BatchNormalization(scale=False, dtype=dtype, fused=False, name='g_bn3')\n",
    "        \n",
    "        # self attention layer\n",
    "        self.attention = SelfAttention(64, dtype=dtype)\n",
    "       \n",
    "        # conv transpose + batch norm layer 4\n",
    "        self.transp_conv4 = Conv2DTranspose(filters=32, kernel_size=4, strides=2,\n",
    "                                            spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                                            padding='same', activation=None, name='g_tr_conv4')\n",
    "        self.bn4 = tf.keras.layers.BatchNormalization(scale=False, dtype=dtype, fused=False, name='g_bn4')\n",
    "#         # conv transpose + batch norm layer 5\n",
    "#         self.transp_conv5 = Conv2DTranspose(filters=16, kernel_size=4, strides=2,\n",
    "#                                             padding='same', activation=None, name='g_tr_conv5')\n",
    "#         self.bn5 = tf.keras.layers.BatchNormalization(scale=False, dtype=dtype, fused=False, name='g_bn5')        \n",
    "        # conv2D\n",
    "        self.conv = Conv2D(filters=3, kernel_size=3, strides=1, dtype=dtype, \n",
    "                           spectral_normalization=APPLY_SPECTRAL_NORM,\n",
    "                           padding='same', activation=None, name='g_conv')\n",
    "        self.out = tf.keras.layers.Activation(activation='tanh', name='g_out')\n",
    "\n",
    "\n",
    "    def call(self, z, is_training):\n",
    "\n",
    "        net = self.fc1(z)\n",
    "        net = tf.reshape(net, (-1,4,4,512), name='g_fc1_reshape')\n",
    "        # first layer operation        \n",
    "        net = self.transp_conv1(net, training=is_training)\n",
    "        net = self.bn1(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        # second layer operation\n",
    "        net = self.transp_conv2(net, training=is_training)\n",
    "        net = self.bn2(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        # third layer operation\n",
    "        net = self.transp_conv3(net, training=is_training)\n",
    "        net = self.bn3(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "        \n",
    "        if APPLY_SELF_ATTENTION:\n",
    "            # self attention operation\n",
    "            net = self.attention(net)\n",
    "        \n",
    "        # fourth layer operation\n",
    "        net = self.transp_conv4(net, training=is_training)\n",
    "        net = self.bn4(net, training=is_training)\n",
    "        net = tf.nn.relu(net)\n",
    "#         # fifth layer operation\n",
    "#         net = self.transp_conv5(net)\n",
    "#         net = self.bn5(net)\n",
    "#         net = tf.nn.relu(net)        \n",
    "        # output layer operation\n",
    "        net = self.conv(net, training=is_training)\n",
    "        output = self.out(net)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def compute_loss(self, logits):\n",
    "#         loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "#                               logits=logits, \n",
    "#                               labels=tf.ones_like(logits)))\n",
    "#         return loss\n",
    "      \n",
    "        return - tf.reduce_mean(d_logits_fake)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLbaIhJLiHQB"
   },
   "source": [
    "## 5 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uG0-jSN2i60I"
   },
   "source": [
    "#### training paramenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D1gDpx4MhHN0"
   },
   "outputs": [],
   "source": [
    "VERSION = \"v1\"\n",
    "MODEL_NAME = \"SAGAN_4dlr_hingeloss_64px_trial2\"\n",
    "MODEL_DIR = \"SAGAN/trained_models/{}\".format(MODEL_NAME)\n",
    "LOG_DIR = os.path.join(MODEL_DIR, 'logs')\n",
    "SRC_TFRECORD_PATH = \"/content/SAGAN/dataset/celeba_dataset\"    # \"SAGAN/dataset/celeba_dataset\"\n",
    "IMG_WIDTH_PIXEL=64\n",
    "IMG_HEIGHT_PIXEL=64\n",
    "IMG_CHANNELS=3\n",
    "Z_SIZE=64\n",
    "SUMMARY_PER_N_STEPS=2500\n",
    "SAVE_MODEL_PER_N_STEPS=2500\n",
    "SAVE_IMG_PER_N_STEPS=1000\n",
    "MAX_TRAINING_STEPS=100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8F1E1tD2jvBW"
   },
   "source": [
    "#### model hyper-paramenters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pfvIR5AlhHRd"
   },
   "outputs": [],
   "source": [
    "# alpha param for leaky ReLu\n",
    "ALPHA = 0.2\n",
    "# learning rates\n",
    "G_LR = 0.0001\n",
    "D_LR = 0.0004\n",
    "# beta params for the Adam optimizer\n",
    "BETA1 = 0.0\n",
    "BETA2 = 0.9\n",
    "# training batch size & epoch\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XcYZZwclk9bI"
   },
   "source": [
    "#### instantiating objects\n",
    "\n",
    "- TF dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQX3GgJok-Nz"
   },
   "outputs": [],
   "source": [
    "dataset = create_tfdataset(tfrecord_file=SRC_TFRECORD_PATH+'.tfrecord',\n",
    "                           img_height=IMG_HEIGHT_PIXEL,\n",
    "                           img_width=IMG_WIDTH_PIXEL,\n",
    "                           shuffle_buffer=512, \n",
    "                           epochs=EPOCHS,\n",
    "                           batch_size=BATCH_SIZE, \n",
    "                           pThreads=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kBnHc7G1l0TB"
   },
   "source": [
    "- Generator & Discriminator model objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dn9JrvC9l6Fr"
   },
   "outputs": [],
   "source": [
    "# generator network\n",
    "g_net = Generator(dtype='float32')\n",
    "# generator optimizer\n",
    "g_optimizer = tf.train.AdamOptimizer(learning_rate=G_LR, \n",
    "                                     beta1=BETA1, \n",
    "                                     beta2=BETA2)\n",
    "\n",
    "# discriminator network\n",
    "d_net = Discriminator(alpha=ALPHA, dtype='float32')\n",
    "# discriminator optimizer\n",
    "d_optimizer = tf.train.AdamOptimizer(learning_rate=D_LR, \n",
    "                                     beta1=BETA1, \n",
    "                                     beta2=BETA2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URFgVSrzmPpf"
   },
   "source": [
    "#### tensorboard summary & checkpoint writers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1BSKkgGDnEbi"
   },
   "outputs": [],
   "source": [
    "# set up tensorboard writer\n",
    "tf_board_writer = tf.contrib.summary.create_file_writer(LOG_DIR)\n",
    "tf_board_writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cfLo4XInmNl"
   },
   "outputs": [],
   "source": [
    "img_sample_dir = os.path.join(MODEL_DIR, 'img_samples')\n",
    "if not os.path.isdir(img_sample_dir):\n",
    "    os.mkdir(img_sample_dir)\n",
    "\n",
    "# set up checkpoint directories\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "# generator checkpoint directory \n",
    "g_checkpoint_dir = os.path.join(MODEL_DIR, 'generator')\n",
    "g_root = tfe.Checkpoint(optimizer=g_optimizer, \n",
    "                        model=g_net,\n",
    "                        optimizer_step=global_step)\n",
    "\n",
    "# discriminator checkpoint directory \n",
    "d_checkpoint_dir = os.path.join(MODEL_DIR, 'discriminator')\n",
    "d_root = tfe.Checkpoint(optimizer=d_optimizer, \n",
    "                        model=d_net,\n",
    "                        optimizer_step=global_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YlBdmWDemQdD"
   },
   "source": [
    "#### load from previous checkpoint (if exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pz4mooxS5Nhv"
   },
   "outputs": [],
   "source": [
    "# copy previous results over from drive\n",
    "!cp -r /content/drive/'My Drive'/colab_notebooks/SAGAN/trained_models/SAGAN_4dlr_hingeloss_64px_trial2 /content/SAGAN/trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1645,
     "status": "ok",
     "timestamp": 1541548650417,
     "user": {
      "displayName": "Jake Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/-qpuoEuHtM6k/AAAAAAAAAAI/AAAAAAAAASM/Yutboujk5B4/s64/photo.jpg",
      "userId": "09150888735248367162"
     },
     "user_tz": 300
    },
    "id": "e_aX5PPjoLYm",
    "outputId": "0d70d062-c83b-41c2-df92-9d749d80f133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming training from latest checkpoint\n",
      "Generator and Discriminator models loaded from global step: 100000\n"
     ]
    }
   ],
   "source": [
    "# restore generator/discriminator from previous checkpoints (if exist)\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    try:\n",
    "        g_root.restore(tf.train.latest_checkpoint(g_checkpoint_dir))\n",
    "    except Exception as ex:\n",
    "        print('Could not load Generator model from {}'.format(g_checkpoint_dir))\n",
    "    try:\n",
    "        d_root.restore(tf.train.latest_checkpoint(d_checkpoint_dir))\n",
    "    except Exception as ex:\n",
    "        print('Could not load Discriminator model from {}'.format(d_checkpoint_dir))\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    print('Resuming training from latest checkpoint')\n",
    "    print('Generator and Discriminator models loaded from global step: {}'.\\\n",
    "          format(tf.train.get_or_create_global_step().numpy()))\n",
    "else:\n",
    "    print('Model folder not found.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gQQRdoZZs6Yq"
   },
   "source": [
    "#### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQdeGyQZRwDc"
   },
   "outputs": [],
   "source": [
    "# # fixed eval_z vector\n",
    "# eval_z = tf.random_normal(shape=(BATCH_SIZE, Z_SIZE), dtype='float32')\n",
    "\n",
    "# # save to file for next time\n",
    "# np.save(img_sample_dir+'/z_input.npy', eval_z.numpy()) \n",
    "\n",
    "# load from file\n",
    "eval_z = np.load(img_sample_dir+'/z_input.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 19564
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15686670,
     "status": "error",
     "timestamp": 1541453002062,
     "user": {
      "displayName": "Jake Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/-qpuoEuHtM6k/AAAAAAAAAAI/AAAAAAAAASM/Yutboujk5B4/s64/photo.jpg",
      "userId": "09150888735248367162"
     },
     "user_tz": 300
    },
    "id": "JlbkKk8Es0-q",
    "outputId": "fe6ec2bc-1940-4df2-9983-bdf8e49da823"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training step 100: discriminator loss 0.5758100152015686; generator loss 0.5085903406143188\n",
      "training step 200: discriminator loss 0.31599318981170654; generator loss 0.7916896939277649\n",
      "training step 300: discriminator loss 1.2474250793457031; generator loss 1.2834632396697998\n",
      "training step 400: discriminator loss 1.3714842796325684; generator loss -0.09806103259325027\n",
      "training step 500: discriminator loss 0.9476824402809143; generator loss 0.504289984703064\n",
      "training step 600: discriminator loss 1.208711862564087; generator loss 0.5330641269683838\n",
      "training step 700: discriminator loss 1.1163430213928223; generator loss 0.20929700136184692\n",
      "training step 800: discriminator loss 0.7843515872955322; generator loss 0.48741888999938965\n",
      "training step 900: discriminator loss 1.3531945943832397; generator loss 0.17101359367370605\n",
      "training step 1000: discriminator loss 1.4328596591949463; generator loss 0.46320053935050964\n",
      "Current step:1000\n",
      "training step 1100: discriminator loss 2.484928607940674; generator loss -1.3286385536193848\n",
      "training step 1200: discriminator loss 1.0793718099594116; generator loss 1.3926935195922852\n",
      "training step 1300: discriminator loss 1.9637271165847778; generator loss -0.8574831485748291\n",
      "training step 1400: discriminator loss 1.628873348236084; generator loss -0.42592495679855347\n",
      "training step 1500: discriminator loss 1.404627799987793; generator loss -0.19806095957756042\n",
      "training step 1600: discriminator loss 1.1834791898727417; generator loss 0.9256412386894226\n",
      "training step 1700: discriminator loss 1.1035665273666382; generator loss 0.3869459629058838\n",
      "training step 1800: discriminator loss 0.6763275265693665; generator loss 0.940285325050354\n",
      "training step 1900: discriminator loss 0.8829343318939209; generator loss 1.111447811126709\n",
      "training step 2000: discriminator loss 0.6372532248497009; generator loss 1.395078420639038\n",
      "Current step:2000\n",
      "training step 2100: discriminator loss 1.0883612632751465; generator loss 0.13647404313087463\n",
      "training step 2200: discriminator loss 1.1417262554168701; generator loss 2.2006850242614746\n",
      "training step 2300: discriminator loss 0.951020359992981; generator loss 1.5151944160461426\n",
      "training step 2400: discriminator loss 1.4024274349212646; generator loss 0.03482604771852493\n",
      "training step 2500: discriminator loss 0.6919676661491394; generator loss 0.737483024597168\n",
      "Saving model snapshot\n",
      "training step 2600: discriminator loss 1.089882731437683; generator loss 2.5356268882751465\n",
      "training step 2700: discriminator loss 0.808077871799469; generator loss 1.2462986707687378\n",
      "training step 2800: discriminator loss 0.716974139213562; generator loss 1.3451486825942993\n",
      "training step 2900: discriminator loss 0.8823846578598022; generator loss 1.686076045036316\n",
      "training step 3000: discriminator loss 1.3561584949493408; generator loss -0.13832660019397736\n",
      "Current step:3000\n",
      "training step 3100: discriminator loss 0.925788164138794; generator loss 0.6823980808258057\n",
      "training step 3200: discriminator loss 0.6653538942337036; generator loss 0.9088550806045532\n",
      "training step 3300: discriminator loss 1.1686651706695557; generator loss 1.3739092350006104\n",
      "training step 3400: discriminator loss 0.8319907188415527; generator loss 1.07918381690979\n",
      "training step 3500: discriminator loss 1.097069501876831; generator loss 0.2273019254207611\n",
      "training step 3600: discriminator loss 1.5675764083862305; generator loss -0.14902791380882263\n",
      "training step 3700: discriminator loss 0.737525224685669; generator loss 1.3160641193389893\n",
      "training step 3800: discriminator loss 1.7662248611450195; generator loss -0.4655575156211853\n",
      "training step 3900: discriminator loss 1.1769356727600098; generator loss 1.1747336387634277\n",
      "training step 4000: discriminator loss 1.2698383331298828; generator loss 0.35113611817359924\n",
      "Current step:4000\n",
      "training step 4100: discriminator loss 1.0439893007278442; generator loss 0.7757134437561035\n",
      "training step 4200: discriminator loss 1.109403371810913; generator loss 1.3783330917358398\n",
      "training step 4300: discriminator loss 0.8690946102142334; generator loss 1.617445707321167\n",
      "training step 4400: discriminator loss 1.57562255859375; generator loss -0.017037559300661087\n",
      "training step 4500: discriminator loss 1.7883851528167725; generator loss -0.6832091212272644\n",
      "training step 4600: discriminator loss 1.1408880949020386; generator loss 0.8114315271377563\n",
      "training step 4700: discriminator loss 1.2916046380996704; generator loss 1.4100345373153687\n",
      "training step 4800: discriminator loss 1.0764200687408447; generator loss 1.359641671180725\n",
      "training step 4900: discriminator loss 1.210213541984558; generator loss 1.7143185138702393\n",
      "training step 5000: discriminator loss 1.4556355476379395; generator loss 0.00838720053434372\n",
      "Current step:5000\n",
      "Saving model snapshot\n",
      "training step 5100: discriminator loss 0.7732886075973511; generator loss 0.8488491773605347\n",
      "training step 5200: discriminator loss 1.2538650035858154; generator loss 0.11466085910797119\n",
      "training step 5300: discriminator loss 1.530164122581482; generator loss -0.26117298007011414\n",
      "training step 5400: discriminator loss 1.2855585813522339; generator loss -0.13291701674461365\n",
      "training step 5500: discriminator loss 1.086178183555603; generator loss 1.590822696685791\n",
      "training step 5600: discriminator loss 1.1374716758728027; generator loss 1.5459296703338623\n",
      "training step 5700: discriminator loss 1.4524065256118774; generator loss 0.0523211695253849\n",
      "training step 5800: discriminator loss 1.3719911575317383; generator loss 1.04757559299469\n",
      "training step 5900: discriminator loss 0.9929943680763245; generator loss 0.8767451047897339\n",
      "training step 6000: discriminator loss 1.1133986711502075; generator loss 1.889714002609253\n",
      "Current step:6000\n",
      "training step 6100: discriminator loss 0.6977605223655701; generator loss 1.0261178016662598\n",
      "training step 6200: discriminator loss 0.8647145628929138; generator loss 0.9843130111694336\n",
      "training step 6300: discriminator loss 1.791032075881958; generator loss -0.4062483012676239\n",
      "training step 6400: discriminator loss 1.1604182720184326; generator loss 0.9695855379104614\n",
      "training step 6500: discriminator loss 0.8987559676170349; generator loss 1.4517606496810913\n",
      "training step 6600: discriminator loss 1.1629767417907715; generator loss -0.008427835069596767\n",
      "training step 6700: discriminator loss 1.104130744934082; generator loss 0.2669036090373993\n",
      "training step 6800: discriminator loss 1.1112260818481445; generator loss 0.18677014112472534\n",
      "training step 6900: discriminator loss 1.3267332315444946; generator loss -0.15312138199806213\n",
      "training step 7000: discriminator loss 1.0032613277435303; generator loss 0.30290210247039795\n",
      "Current step:7000\n",
      "training step 7100: discriminator loss 1.0310771465301514; generator loss 0.4359263777732849\n",
      "training step 7200: discriminator loss 0.7728785276412964; generator loss 1.3131380081176758\n",
      "training step 7300: discriminator loss 0.8793405294418335; generator loss 1.4951741695404053\n",
      "training step 7400: discriminator loss 1.3066279888153076; generator loss -0.07496131956577301\n",
      "training step 7500: discriminator loss 0.6883169412612915; generator loss 1.396744966506958\n",
      "Saving model snapshot\n",
      "training step 7600: discriminator loss 1.06400465965271; generator loss 1.2421092987060547\n",
      "training step 7700: discriminator loss 1.4602926969528198; generator loss 1.9484398365020752\n",
      "training step 7800: discriminator loss 0.8723737001419067; generator loss 2.407139539718628\n",
      "training step 7900: discriminator loss 1.1838111877441406; generator loss 0.0909636840224266\n",
      "training step 8000: discriminator loss 1.5030418634414673; generator loss 2.063018560409546\n",
      "Current step:8000\n",
      "training step 8100: discriminator loss 0.7519259452819824; generator loss 1.535667896270752\n",
      "training step 8200: discriminator loss 0.6018822193145752; generator loss 1.0361889600753784\n",
      "training step 8300: discriminator loss 0.7420991659164429; generator loss 0.7118290066719055\n",
      "training step 8400: discriminator loss 0.8634378910064697; generator loss 1.21824049949646\n",
      "training step 8500: discriminator loss 0.7131390571594238; generator loss 1.4624924659729004\n",
      "training step 8600: discriminator loss 0.7790295481681824; generator loss 0.5115842819213867\n",
      "training step 8700: discriminator loss 1.2319085597991943; generator loss 0.23701314628124237\n",
      "training step 8800: discriminator loss 0.8994513154029846; generator loss 1.7454655170440674\n",
      "training step 8900: discriminator loss 0.8629812002182007; generator loss 1.9012541770935059\n",
      "training step 9000: discriminator loss 0.628121554851532; generator loss 1.9813611507415771\n",
      "Current step:9000\n",
      "training step 9100: discriminator loss 0.7901480793952942; generator loss 1.5961782932281494\n",
      "training step 9200: discriminator loss 1.5819189548492432; generator loss 0.01067495159804821\n",
      "training step 9300: discriminator loss 3.9342663288116455; generator loss -2.866034984588623\n",
      "training step 9400: discriminator loss 1.2965822219848633; generator loss 0.4168925881385803\n",
      "training step 9500: discriminator loss 1.2273523807525635; generator loss 0.12579023838043213\n",
      "training step 9600: discriminator loss 0.8722641468048096; generator loss 0.7278605103492737\n",
      "training step 9700: discriminator loss 1.122799277305603; generator loss 0.2378726303577423\n",
      "training step 9800: discriminator loss 1.6490108966827393; generator loss 2.4819185733795166\n",
      "training step 9900: discriminator loss 1.3188509941101074; generator loss 0.05039405822753906\n",
      "training step 10000: discriminator loss 0.8409836292266846; generator loss 0.6920333504676819\n",
      "Current step:10000\n",
      "Saving model snapshot\n",
      "training step 10100: discriminator loss 1.2902376651763916; generator loss 2.1372711658477783\n",
      "training step 10200: discriminator loss 1.1775197982788086; generator loss 1.6142892837524414\n",
      "training step 10300: discriminator loss 1.0601593255996704; generator loss 0.294181764125824\n",
      "training step 10400: discriminator loss 1.1605812311172485; generator loss 1.4822365045547485\n",
      "training step 10500: discriminator loss 1.0836780071258545; generator loss 1.929335594177246\n",
      "training step 10600: discriminator loss 0.8730854988098145; generator loss 0.5718338489532471\n",
      "training step 10700: discriminator loss 1.1168354749679565; generator loss 0.3441656529903412\n",
      "training step 10800: discriminator loss 0.7532215118408203; generator loss 2.8658359050750732\n",
      "training step 10900: discriminator loss 0.5939268469810486; generator loss 0.6603371500968933\n",
      "training step 11000: discriminator loss 1.0148507356643677; generator loss 1.7335665225982666\n",
      "Current step:11000\n",
      "training step 11100: discriminator loss 0.8572563529014587; generator loss 1.5958771705627441\n",
      "training step 11200: discriminator loss 1.2710151672363281; generator loss 0.21930578351020813\n",
      "training step 11300: discriminator loss 0.8739565014839172; generator loss 0.38133037090301514\n",
      "training step 11400: discriminator loss 1.2064721584320068; generator loss 0.22786620259284973\n",
      "training step 11500: discriminator loss 1.320609450340271; generator loss 0.5179355144500732\n",
      "training step 11600: discriminator loss 0.5110394954681396; generator loss 0.9169263243675232\n",
      "training step 11700: discriminator loss 0.7305031418800354; generator loss 2.3998236656188965\n",
      "training step 11800: discriminator loss 1.4896855354309082; generator loss 1.757372260093689\n",
      "training step 11900: discriminator loss 0.8634341955184937; generator loss 0.4833855926990509\n",
      "training step 12000: discriminator loss 0.701728880405426; generator loss 1.4670699834823608\n",
      "Current step:12000\n",
      "training step 12100: discriminator loss 0.4358225166797638; generator loss 0.8785191774368286\n",
      "training step 12200: discriminator loss 1.7063066959381104; generator loss -0.6091052293777466\n",
      "training step 12300: discriminator loss 1.954308032989502; generator loss -0.7985118627548218\n",
      "training step 12400: discriminator loss 0.6079514026641846; generator loss 0.95744788646698\n",
      "training step 12500: discriminator loss 1.331270694732666; generator loss -0.14102482795715332\n",
      "Saving model snapshot\n",
      "training step 12600: discriminator loss 1.6447868347167969; generator loss -0.395325243473053\n",
      "training step 12700: discriminator loss 0.925876259803772; generator loss 2.7797536849975586\n",
      "training step 12800: discriminator loss 0.956584632396698; generator loss 0.3444398045539856\n",
      "training step 12900: discriminator loss 0.8949563503265381; generator loss 1.2002652883529663\n",
      "training step 13000: discriminator loss 1.0444029569625854; generator loss 2.7329983711242676\n",
      "Current step:13000\n",
      "training step 13100: discriminator loss 0.8714429140090942; generator loss 1.7427818775177002\n",
      "training step 13200: discriminator loss 1.2134599685668945; generator loss 2.9174447059631348\n",
      "training step 13300: discriminator loss 0.9389362335205078; generator loss 0.2726781964302063\n",
      "training step 13400: discriminator loss 1.1616113185882568; generator loss 0.28364282846450806\n",
      "training step 13500: discriminator loss 0.8849185705184937; generator loss 1.5571887493133545\n",
      "training step 13600: discriminator loss 1.28846275806427; generator loss 1.0778346061706543\n",
      "training step 13700: discriminator loss 0.7283573150634766; generator loss 2.381865978240967\n",
      "training step 13800: discriminator loss 1.0846434831619263; generator loss 0.22036181390285492\n",
      "training step 13900: discriminator loss 0.9826713800430298; generator loss 1.810777187347412\n",
      "training step 14000: discriminator loss 1.3114794492721558; generator loss 2.5467097759246826\n",
      "Current step:14000\n",
      "training step 14100: discriminator loss 1.086288332939148; generator loss 2.3794922828674316\n",
      "training step 14200: discriminator loss 2.201906681060791; generator loss -0.7858137488365173\n",
      "training step 14300: discriminator loss 0.981217622756958; generator loss 3.189358711242676\n",
      "training step 14400: discriminator loss 0.7429693937301636; generator loss 2.2576093673706055\n",
      "training step 14500: discriminator loss 0.8097974061965942; generator loss 0.3777822256088257\n",
      "training step 14600: discriminator loss 0.9723600149154663; generator loss 0.49805986881256104\n",
      "training step 14700: discriminator loss 1.5200706720352173; generator loss 2.5056161880493164\n",
      "training step 14800: discriminator loss 1.6967501640319824; generator loss -0.3203347325325012\n",
      "training step 14900: discriminator loss 1.290824294090271; generator loss -0.06257551908493042\n",
      "training step 15000: discriminator loss 1.1997276544570923; generator loss 3.388187885284424\n",
      "Current step:15000\n",
      "Saving model snapshot\n",
      "training step 15100: discriminator loss 1.4368047714233398; generator loss 0.3495059609413147\n",
      "training step 15200: discriminator loss 0.939062774181366; generator loss 0.41890618205070496\n",
      "training step 15300: discriminator loss 1.0835542678833008; generator loss 0.7948535680770874\n",
      "training step 15400: discriminator loss 1.2072169780731201; generator loss 2.5018458366394043\n",
      "training step 15500: discriminator loss 1.1500886678695679; generator loss 1.9339861869812012\n",
      "training step 15600: discriminator loss 1.3509650230407715; generator loss 0.0354224368929863\n",
      "training step 15700: discriminator loss 1.558004379272461; generator loss 0.049958255141973495\n",
      "training step 15800: discriminator loss 1.189494013786316; generator loss 1.7023345232009888\n",
      "training step 15900: discriminator loss 0.936622142791748; generator loss 1.8315751552581787\n",
      "training step 16000: discriminator loss 0.7326940894126892; generator loss 2.384176731109619\n",
      "Current step:16000\n",
      "training step 16100: discriminator loss 3.2799229621887207; generator loss -2.1707568168640137\n",
      "training step 16200: discriminator loss 1.1521626710891724; generator loss 1.38155996799469\n",
      "training step 16300: discriminator loss 0.9453116655349731; generator loss 2.2226455211639404\n",
      "training step 16400: discriminator loss 1.278795599937439; generator loss 2.7338080406188965\n",
      "training step 16500: discriminator loss 1.4313526153564453; generator loss 0.5005913972854614\n",
      "training step 16600: discriminator loss 1.1594603061676025; generator loss 0.0989341139793396\n",
      "training step 16700: discriminator loss 1.5128778219223022; generator loss -0.17457859218120575\n",
      "training step 16800: discriminator loss 1.157747507095337; generator loss 0.5719625949859619\n",
      "training step 16900: discriminator loss 1.9830092191696167; generator loss -0.7869155406951904\n",
      "training step 17000: discriminator loss 1.0484943389892578; generator loss 0.735783576965332\n",
      "Current step:17000\n",
      "training step 17100: discriminator loss 0.27143898606300354; generator loss 4.072851181030273\n",
      "training step 17200: discriminator loss 1.9570788145065308; generator loss -0.6428283452987671\n",
      "training step 17300: discriminator loss 1.4767587184906006; generator loss -0.07476416230201721\n",
      "training step 17400: discriminator loss 1.9754841327667236; generator loss 3.5866074562072754\n",
      "training step 17500: discriminator loss 1.6659386157989502; generator loss -0.4613511562347412\n",
      "Saving model snapshot\n",
      "training step 17600: discriminator loss 1.3786258697509766; generator loss 0.009660482406616211\n",
      "training step 17700: discriminator loss 1.473258376121521; generator loss 0.054777562618255615\n",
      "training step 17800: discriminator loss 1.3038684129714966; generator loss 2.5964176654815674\n",
      "training step 17900: discriminator loss 1.4957029819488525; generator loss -0.018155381083488464\n",
      "training step 18000: discriminator loss 0.9273216128349304; generator loss 1.5947353839874268\n",
      "Current step:18000\n",
      "training step 18100: discriminator loss 0.9681257605552673; generator loss 2.504565477371216\n",
      "training step 18200: discriminator loss 2.901730537414551; generator loss -1.5908417701721191\n",
      "training step 18300: discriminator loss 3.1673288345336914; generator loss -1.8657772541046143\n",
      "training step 18400: discriminator loss 2.371567964553833; generator loss -1.2735490798950195\n",
      "training step 18500: discriminator loss 1.538028597831726; generator loss -0.3714817464351654\n",
      "training step 18600: discriminator loss 1.779909372329712; generator loss -0.18304744362831116\n",
      "training step 18700: discriminator loss 1.097270131111145; generator loss 1.9743239879608154\n",
      "training step 18800: discriminator loss 1.186537742614746; generator loss 1.3203766345977783\n",
      "training step 18900: discriminator loss 1.2190558910369873; generator loss 0.6895170211791992\n",
      "training step 19000: discriminator loss 0.8476660847663879; generator loss 0.8345078229904175\n",
      "Current step:19000\n",
      "training step 19100: discriminator loss 0.3510225713253021; generator loss 3.397739887237549\n",
      "training step 19200: discriminator loss 0.7423180341720581; generator loss 1.9413809776306152\n",
      "training step 19300: discriminator loss 1.2967640161514282; generator loss 0.19254310429096222\n",
      "training step 19400: discriminator loss 1.5048161745071411; generator loss 1.3189998865127563\n",
      "training step 19500: discriminator loss 1.3884910345077515; generator loss 1.2516688108444214\n",
      "training step 19600: discriminator loss 1.2071518898010254; generator loss 2.080702781677246\n",
      "training step 19700: discriminator loss 1.550540566444397; generator loss 2.526568651199341\n",
      "training step 19800: discriminator loss 1.2512964010238647; generator loss 2.1643803119659424\n",
      "training step 19900: discriminator loss 1.4980707168579102; generator loss 1.6969106197357178\n",
      "training step 20000: discriminator loss 3.1183221340179443; generator loss -1.8859553337097168\n",
      "Current step:20000\n",
      "Saving model snapshot\n",
      "training step 20100: discriminator loss 1.7102299928665161; generator loss -0.43495872616767883\n",
      "training step 20200: discriminator loss 1.7050288915634155; generator loss 2.3111281394958496\n",
      "training step 20300: discriminator loss 1.1099580526351929; generator loss 1.230638861656189\n",
      "training step 20400: discriminator loss 1.2909914255142212; generator loss 2.131133556365967\n",
      "training step 20500: discriminator loss 1.4185764789581299; generator loss 0.1321476399898529\n",
      "training step 20600: discriminator loss 1.0616692304611206; generator loss 3.863302707672119\n",
      "training step 20700: discriminator loss 1.2675015926361084; generator loss 3.8633623123168945\n",
      "training step 20800: discriminator loss 1.642085313796997; generator loss 2.5235655307769775\n",
      "training step 20900: discriminator loss 2.789121389389038; generator loss -1.0175459384918213\n",
      "training step 21000: discriminator loss 1.0815694332122803; generator loss 0.8778895139694214\n",
      "Current step:21000\n",
      "training step 21100: discriminator loss 0.6897814869880676; generator loss 2.6929569244384766\n",
      "training step 21200: discriminator loss 1.6406193971633911; generator loss -0.14577239751815796\n",
      "training step 21300: discriminator loss 1.4380953311920166; generator loss 0.06461740285158157\n",
      "training step 21400: discriminator loss 1.8325474262237549; generator loss -0.49103879928588867\n",
      "training step 21500: discriminator loss 1.6902928352355957; generator loss -0.1575915515422821\n",
      "training step 21600: discriminator loss 1.7794559001922607; generator loss 2.302293300628662\n",
      "training step 21700: discriminator loss 1.629326343536377; generator loss 2.9131698608398438\n",
      "training step 21800: discriminator loss 1.007164716720581; generator loss 2.531627893447876\n",
      "training step 21900: discriminator loss 1.3530559539794922; generator loss 0.5770729780197144\n",
      "training step 22000: discriminator loss 1.5408945083618164; generator loss 2.0657944679260254\n",
      "Current step:22000\n",
      "training step 22100: discriminator loss 2.1865694522857666; generator loss -0.7849048972129822\n",
      "training step 22200: discriminator loss 1.0973939895629883; generator loss 2.286806106567383\n",
      "training step 22300: discriminator loss 1.0816861391067505; generator loss 1.3012306690216064\n",
      "training step 22400: discriminator loss 1.1966272592544556; generator loss 1.5900911092758179\n",
      "training step 22500: discriminator loss 1.0780744552612305; generator loss 1.3922538757324219\n",
      "Saving model snapshot\n",
      "training step 22600: discriminator loss 2.01713228225708; generator loss -0.4265441596508026\n",
      "training step 22700: discriminator loss 2.334230422973633; generator loss -0.6810263395309448\n",
      "training step 22800: discriminator loss 1.4856764078140259; generator loss -0.12361523509025574\n",
      "training step 22900: discriminator loss 1.4033010005950928; generator loss 1.060853362083435\n",
      "training step 23000: discriminator loss 1.1625343561172485; generator loss 1.6807981729507446\n",
      "Current step:23000\n",
      "training step 23100: discriminator loss 1.646791696548462; generator loss 1.8698110580444336\n",
      "training step 23200: discriminator loss 1.5960568189620972; generator loss -0.03509359061717987\n",
      "training step 23300: discriminator loss 1.0054104328155518; generator loss 0.6564904451370239\n",
      "training step 23400: discriminator loss 0.8144536018371582; generator loss 1.7550013065338135\n",
      "training step 23500: discriminator loss 1.3413200378417969; generator loss 2.101872205734253\n",
      "training step 23600: discriminator loss 1.517671823501587; generator loss -0.3059006631374359\n",
      "training step 23700: discriminator loss 1.9177570343017578; generator loss -0.5906236171722412\n",
      "training step 23800: discriminator loss 1.2658429145812988; generator loss 0.37795722484588623\n",
      "training step 23900: discriminator loss 1.6868317127227783; generator loss -0.20611853897571564\n",
      "training step 24000: discriminator loss 1.9404706954956055; generator loss -0.2858228087425232\n",
      "Current step:24000\n",
      "training step 24100: discriminator loss 0.9132018089294434; generator loss 0.5467087626457214\n",
      "training step 24200: discriminator loss 1.1683589220046997; generator loss 0.3056274652481079\n",
      "training step 24300: discriminator loss 1.4430733919143677; generator loss 2.4624686241149902\n",
      "training step 24400: discriminator loss 1.5099281072616577; generator loss 2.749746322631836\n",
      "training step 24500: discriminator loss 1.3223797082901; generator loss 1.8369231224060059\n",
      "training step 24600: discriminator loss 1.603280782699585; generator loss -0.030337437987327576\n",
      "training step 24700: discriminator loss 2.347261428833008; generator loss -1.2561575174331665\n",
      "training step 24800: discriminator loss 1.5831689834594727; generator loss -0.5430372357368469\n",
      "training step 24900: discriminator loss 1.5241355895996094; generator loss -0.23160019516944885\n",
      "training step 25000: discriminator loss 0.7932614088058472; generator loss 0.9461114406585693\n",
      "Current step:25000\n",
      "Saving model snapshot\n",
      "training step 25100: discriminator loss 1.1334147453308105; generator loss 0.40157872438430786\n",
      "training step 25200: discriminator loss 1.0674469470977783; generator loss 0.3550695478916168\n",
      "training step 25300: discriminator loss 2.205115795135498; generator loss -0.8281887769699097\n",
      "training step 25400: discriminator loss 0.9318369030952454; generator loss 2.3949954509735107\n",
      "training step 25500: discriminator loss 2.5489397048950195; generator loss -1.3777234554290771\n",
      "training step 25600: discriminator loss 1.2661387920379639; generator loss -0.03842523694038391\n",
      "training step 25700: discriminator loss 1.6677577495574951; generator loss -0.2821446657180786\n",
      "training step 25800: discriminator loss 1.0927749872207642; generator loss 1.2633755207061768\n",
      "training step 25900: discriminator loss 1.7568103075027466; generator loss -0.38187462091445923\n",
      "training step 26000: discriminator loss 1.1799196004867554; generator loss 0.9582544565200806\n",
      "Current step:26000\n",
      "training step 26100: discriminator loss 1.4039257764816284; generator loss 1.5184028148651123\n",
      "training step 26200: discriminator loss 1.3630075454711914; generator loss 0.15107788145542145\n",
      "training step 26300: discriminator loss 1.8628926277160645; generator loss -0.5215545892715454\n",
      "training step 26400: discriminator loss 1.4730308055877686; generator loss 1.9006987810134888\n",
      "training step 26500: discriminator loss 1.2278169393539429; generator loss 0.13429932296276093\n",
      "training step 26600: discriminator loss 1.2019672393798828; generator loss 0.21454039216041565\n",
      "training step 26700: discriminator loss 1.5801270008087158; generator loss -0.3396100103855133\n",
      "training step 26800: discriminator loss 1.0988848209381104; generator loss 0.4784913659095764\n",
      "training step 26900: discriminator loss 1.4386835098266602; generator loss 2.7433969974517822\n",
      "training step 27000: discriminator loss 1.6853512525558472; generator loss -0.35710400342941284\n",
      "Current step:27000\n",
      "training step 27100: discriminator loss 2.094841241836548; generator loss -0.9609354734420776\n",
      "training step 27200: discriminator loss 1.3874313831329346; generator loss 0.1514565348625183\n",
      "training step 27300: discriminator loss 1.7368050813674927; generator loss -0.516910970211029\n",
      "training step 27400: discriminator loss 1.265249490737915; generator loss 1.1895411014556885\n",
      "training step 27500: discriminator loss 1.5574921369552612; generator loss -0.07231277227401733\n",
      "Saving model snapshot\n",
      "training step 27600: discriminator loss 1.2354843616485596; generator loss 1.9681241512298584\n",
      "training step 27700: discriminator loss 1.1643853187561035; generator loss 1.5447889566421509\n",
      "training step 27800: discriminator loss 1.047404170036316; generator loss 0.35318273305892944\n",
      "training step 27900: discriminator loss 1.0781933069229126; generator loss 1.0079271793365479\n",
      "training step 28000: discriminator loss 1.1713311672210693; generator loss 2.3338077068328857\n",
      "Current step:28000\n",
      "training step 28100: discriminator loss 1.2800617218017578; generator loss -0.05457872152328491\n",
      "training step 28200: discriminator loss 1.1461113691329956; generator loss 0.26252660155296326\n",
      "training step 28300: discriminator loss 1.132223129272461; generator loss 1.5970971584320068\n",
      "training step 28400: discriminator loss 1.2989039421081543; generator loss 0.06839541345834732\n",
      "training step 28500: discriminator loss 1.8123688697814941; generator loss -0.5380544662475586\n",
      "training step 28600: discriminator loss 0.8712145090103149; generator loss 1.4088789224624634\n",
      "training step 28700: discriminator loss 1.1297006607055664; generator loss 1.2298924922943115\n",
      "training step 28800: discriminator loss 1.1518912315368652; generator loss 0.7401790618896484\n",
      "training step 28900: discriminator loss 1.1749131679534912; generator loss 0.21297650039196014\n",
      "training step 29000: discriminator loss 1.013031005859375; generator loss 0.7772995829582214\n",
      "Current step:29000\n",
      "training step 29100: discriminator loss 1.1907081604003906; generator loss 1.9670662879943848\n",
      "training step 29200: discriminator loss 1.1456730365753174; generator loss 0.5730583667755127\n",
      "training step 29300: discriminator loss 1.7168891429901123; generator loss 0.32389193773269653\n",
      "training step 29400: discriminator loss 1.5331566333770752; generator loss 1.5659945011138916\n",
      "training step 29500: discriminator loss 1.7560397386550903; generator loss -0.42901167273521423\n",
      "training step 29600: discriminator loss 0.9389604330062866; generator loss 0.5865516662597656\n",
      "training step 29700: discriminator loss 1.6428619623184204; generator loss -0.24874429404735565\n",
      "training step 29800: discriminator loss 1.3583334684371948; generator loss 1.2171165943145752\n",
      "training step 29900: discriminator loss 1.1906800270080566; generator loss 1.704573154449463\n",
      "training step 30000: discriminator loss 1.318420648574829; generator loss 0.13351257145404816\n",
      "Current step:30000\n",
      "Saving model snapshot\n",
      "training step 30100: discriminator loss 2.729706048965454; generator loss -1.5922605991363525\n",
      "training step 30200: discriminator loss 1.3389201164245605; generator loss 1.5687894821166992\n",
      "training step 30300: discriminator loss 1.608796238899231; generator loss -0.30898118019104004\n",
      "training step 30400: discriminator loss 1.8627227544784546; generator loss -0.29537081718444824\n",
      "training step 30500: discriminator loss 1.9233458042144775; generator loss -0.6947788000106812\n",
      "training step 30600: discriminator loss 1.3171749114990234; generator loss 0.503636360168457\n",
      "training step 30700: discriminator loss 1.498316764831543; generator loss -0.18497443199157715\n",
      "training step 30800: discriminator loss 1.3999956846237183; generator loss 1.3552451133728027\n",
      "training step 30900: discriminator loss 1.6207926273345947; generator loss -0.3609536588191986\n",
      "training step 31000: discriminator loss 1.4876220226287842; generator loss 1.6022893190383911\n",
      "Current step:31000\n",
      "training step 31100: discriminator loss 1.0701043605804443; generator loss 0.5756116509437561\n",
      "training step 31200: discriminator loss 0.9579672813415527; generator loss 0.7347419261932373\n",
      "training step 31300: discriminator loss 1.4972224235534668; generator loss 1.5552449226379395\n",
      "training step 31400: discriminator loss 1.5852646827697754; generator loss 1.1183815002441406\n",
      "training step 31500: discriminator loss 1.6715112924575806; generator loss 1.1095619201660156\n",
      "training step 31600: discriminator loss 1.257071852684021; generator loss 0.018737301230430603\n",
      "training step 31700: discriminator loss 1.3084540367126465; generator loss 0.9322197437286377\n",
      "training step 31800: discriminator loss 1.0943324565887451; generator loss 1.1078420877456665\n",
      "training step 31900: discriminator loss 1.226312279701233; generator loss 1.1222559213638306\n",
      "training step 32000: discriminator loss 1.2435810565948486; generator loss 1.3590092658996582\n",
      "Current step:32000\n",
      "training step 32100: discriminator loss 1.759831428527832; generator loss -0.3064461946487427\n",
      "training step 32200: discriminator loss 1.6624104976654053; generator loss -0.45473459362983704\n",
      "training step 32300: discriminator loss 1.2525811195373535; generator loss 1.168524146080017\n",
      "training step 32400: discriminator loss 1.1450533866882324; generator loss 1.1413249969482422\n",
      "training step 32500: discriminator loss 1.304764747619629; generator loss 1.3401869535446167\n",
      "Saving model snapshot\n",
      "training step 32600: discriminator loss 1.3747715950012207; generator loss 0.2823691666126251\n",
      "training step 32700: discriminator loss 1.5736558437347412; generator loss -0.2319844514131546\n",
      "training step 32800: discriminator loss 1.8181729316711426; generator loss 1.8927366733551025\n",
      "training step 32900: discriminator loss 1.4489259719848633; generator loss 1.2714319229125977\n",
      "training step 33000: discriminator loss 1.6134734153747559; generator loss 1.3887966871261597\n",
      "Current step:33000\n",
      "training step 33100: discriminator loss 1.3930273056030273; generator loss 1.0995972156524658\n",
      "training step 33200: discriminator loss 2.732332229614258; generator loss -1.6310244798660278\n",
      "training step 33300: discriminator loss 1.2173330783843994; generator loss 0.4154026508331299\n",
      "training step 33400: discriminator loss 1.4201245307922363; generator loss -0.011892668902873993\n",
      "training step 33500: discriminator loss 1.522237777709961; generator loss 0.20210300385951996\n",
      "training step 33600: discriminator loss 1.4048519134521484; generator loss 1.0766569375991821\n",
      "training step 33700: discriminator loss 1.3000268936157227; generator loss 1.7011232376098633\n",
      "training step 33800: discriminator loss 1.9501698017120361; generator loss -0.6959806680679321\n",
      "training step 33900: discriminator loss 1.3923135995864868; generator loss -0.09724555909633636\n",
      "training step 34000: discriminator loss 1.6250033378601074; generator loss 0.38246744871139526\n",
      "Current step:34000\n",
      "training step 34100: discriminator loss 1.5122357606887817; generator loss 1.3206900358200073\n",
      "training step 34200: discriminator loss 1.254399299621582; generator loss 0.6168912649154663\n",
      "training step 34300: discriminator loss 1.4482630491256714; generator loss -0.2987681031227112\n",
      "training step 34400: discriminator loss 1.672448754310608; generator loss 0.038290657103061676\n",
      "training step 34500: discriminator loss 1.4012649059295654; generator loss 0.030075401067733765\n",
      "training step 34600: discriminator loss 1.2192833423614502; generator loss 1.6150410175323486\n",
      "training step 34700: discriminator loss 1.4350985288619995; generator loss 1.6426467895507812\n",
      "training step 34800: discriminator loss 1.5374760627746582; generator loss 0.6776270866394043\n",
      "training step 34900: discriminator loss 1.6683435440063477; generator loss -0.32418960332870483\n",
      "training step 35000: discriminator loss 1.3347032070159912; generator loss 0.24128426611423492\n",
      "Current step:35000\n",
      "Saving model snapshot\n",
      "training step 35100: discriminator loss 1.459812045097351; generator loss 1.1693066358566284\n",
      "training step 35200: discriminator loss 1.2408217191696167; generator loss 1.1067876815795898\n",
      "training step 35300: discriminator loss 1.2775465250015259; generator loss 0.21100154519081116\n",
      "training step 35400: discriminator loss 1.3809785842895508; generator loss 1.4382619857788086\n",
      "training step 35500: discriminator loss 1.073994517326355; generator loss 0.4534836411476135\n",
      "training step 35600: discriminator loss 1.5455985069274902; generator loss 0.8693385124206543\n",
      "training step 35700: discriminator loss 0.95340496301651; generator loss 0.6593866944313049\n",
      "training step 35800: discriminator loss 1.4987547397613525; generator loss 0.0891462042927742\n",
      "training step 35900: discriminator loss 1.6912133693695068; generator loss 1.5494599342346191\n",
      "training step 36000: discriminator loss 1.317939043045044; generator loss -0.12104947865009308\n",
      "Current step:36000\n",
      "training step 36100: discriminator loss 2.018742322921753; generator loss -0.6302030086517334\n",
      "training step 36200: discriminator loss 1.2785454988479614; generator loss 1.202048659324646\n",
      "training step 36300: discriminator loss 1.2110559940338135; generator loss 1.132942795753479\n",
      "training step 36400: discriminator loss 1.3733631372451782; generator loss 0.9725872874259949\n",
      "training step 36500: discriminator loss 1.122044563293457; generator loss 0.3620731234550476\n",
      "training step 36600: discriminator loss 1.398871660232544; generator loss 0.0685117095708847\n",
      "training step 36700: discriminator loss 2.150045394897461; generator loss -0.8606860637664795\n",
      "training step 36800: discriminator loss 1.7754054069519043; generator loss 1.7148120403289795\n",
      "training step 36900: discriminator loss 0.9247003793716431; generator loss 0.5533369779586792\n",
      "training step 37000: discriminator loss 1.5755650997161865; generator loss -0.18001478910446167\n",
      "Current step:37000\n",
      "training step 37100: discriminator loss 1.1909464597702026; generator loss 1.063446283340454\n",
      "training step 37200: discriminator loss 1.1865311861038208; generator loss 0.873077392578125\n",
      "training step 37300: discriminator loss 1.6391332149505615; generator loss -0.14402218163013458\n",
      "training step 37400: discriminator loss 1.774635672569275; generator loss 1.4712529182434082\n",
      "training step 37500: discriminator loss 1.5054857730865479; generator loss 0.9891113638877869\n",
      "Saving model snapshot\n",
      "training step 37600: discriminator loss 2.484238624572754; generator loss -1.0640596151351929\n",
      "training step 37700: discriminator loss 1.2844603061676025; generator loss 0.40721753239631653\n",
      "training step 37800: discriminator loss 1.3403267860412598; generator loss 0.8969169855117798\n",
      "training step 37900: discriminator loss 1.2247674465179443; generator loss 0.9228062033653259\n",
      "training step 38000: discriminator loss 1.5340852737426758; generator loss 0.0651223212480545\n",
      "Current step:38000\n",
      "training step 38100: discriminator loss 1.2048256397247314; generator loss 0.5360884070396423\n",
      "training step 38200: discriminator loss 1.5115692615509033; generator loss 0.07703766226768494\n",
      "training step 38300: discriminator loss 1.5575200319290161; generator loss 0.09261933714151382\n",
      "training step 38400: discriminator loss 1.581024408340454; generator loss 1.0474426746368408\n",
      "training step 38500: discriminator loss 1.4963610172271729; generator loss 1.3781050443649292\n",
      "training step 38600: discriminator loss 1.6922001838684082; generator loss -0.2446700930595398\n",
      "training step 38700: discriminator loss 1.3735228776931763; generator loss 0.4335922598838806\n",
      "training step 38800: discriminator loss 1.58811354637146; generator loss 1.4872124195098877\n",
      "training step 38900: discriminator loss 1.7371141910552979; generator loss -0.08629487454891205\n",
      "training step 39000: discriminator loss 1.4807116985321045; generator loss 1.3091603517532349\n",
      "Current step:39000\n",
      "training step 39100: discriminator loss 1.3481565713882446; generator loss 0.6973191499710083\n",
      "training step 39200: discriminator loss 1.2825855016708374; generator loss 1.0085002183914185\n",
      "training step 39300: discriminator loss 2.0048940181732178; generator loss -0.7617334127426147\n",
      "training step 39400: discriminator loss 1.5058284997940063; generator loss 0.5441514253616333\n",
      "training step 39500: discriminator loss 1.543614149093628; generator loss 0.9292187690734863\n",
      "training step 39600: discriminator loss 1.6944046020507812; generator loss -0.4168948531150818\n",
      "training step 39700: discriminator loss 1.5178442001342773; generator loss 1.1277024745941162\n",
      "training step 39800: discriminator loss 1.2142668962478638; generator loss 1.0372610092163086\n",
      "training step 39900: discriminator loss 1.4954637289047241; generator loss 1.3718981742858887\n",
      "training step 40000: discriminator loss 1.3240338563919067; generator loss 1.1960766315460205\n",
      "Current step:40000\n",
      "Saving model snapshot\n",
      "training step 40100: discriminator loss 1.3947077989578247; generator loss 1.4579236507415771\n",
      "training step 40200: discriminator loss 1.4301732778549194; generator loss -0.03976285830140114\n",
      "training step 40300: discriminator loss 1.3447614908218384; generator loss 1.2704737186431885\n",
      "training step 40400: discriminator loss 1.3578903675079346; generator loss 0.6155197620391846\n",
      "training step 40500: discriminator loss 1.520510196685791; generator loss 1.6183160543441772\n",
      "training step 40600: discriminator loss 1.1412601470947266; generator loss 0.46175020933151245\n",
      "training step 40700: discriminator loss 1.2062764167785645; generator loss 0.6379480361938477\n",
      "training step 40800: discriminator loss 1.3011246919631958; generator loss 0.42570674419403076\n",
      "training step 40900: discriminator loss 1.2466150522232056; generator loss 0.7575518488883972\n",
      "training step 41000: discriminator loss 1.1943416595458984; generator loss 1.025876522064209\n",
      "Current step:41000\n",
      "training step 41100: discriminator loss 1.5152169466018677; generator loss -0.12930312752723694\n",
      "training step 41200: discriminator loss 1.6926989555358887; generator loss 1.0048716068267822\n",
      "training step 41300: discriminator loss 1.4567903280258179; generator loss 1.2565428018569946\n",
      "training step 41400: discriminator loss 1.3604905605316162; generator loss 0.1006426066160202\n",
      "training step 41500: discriminator loss 1.8023022413253784; generator loss 1.562589406967163\n",
      "training step 41600: discriminator loss 1.5449116230010986; generator loss 1.0866680145263672\n",
      "training step 41700: discriminator loss 1.4660005569458008; generator loss 0.39135605096817017\n",
      "training step 41800: discriminator loss 1.7976922988891602; generator loss -0.3451040983200073\n",
      "training step 41900: discriminator loss 1.535177230834961; generator loss 0.6909570693969727\n",
      "training step 42000: discriminator loss 1.4053196907043457; generator loss 1.245649814605713\n",
      "Current step:42000\n",
      "training step 42100: discriminator loss 1.3436954021453857; generator loss 1.0673933029174805\n",
      "training step 42200: discriminator loss 1.3206348419189453; generator loss 0.8580844402313232\n",
      "training step 42300: discriminator loss 1.28286612033844; generator loss 1.1776623725891113\n",
      "training step 42400: discriminator loss 1.803442120552063; generator loss -0.3358471393585205\n",
      "training step 42500: discriminator loss 1.6200604438781738; generator loss 0.06429465115070343\n",
      "Saving model snapshot\n",
      "training step 42600: discriminator loss 1.402944803237915; generator loss -0.08311408013105392\n",
      "training step 42700: discriminator loss 1.5629494190216064; generator loss 0.04726368188858032\n",
      "training step 42800: discriminator loss 1.6810169219970703; generator loss -0.3705217242240906\n",
      "training step 42900: discriminator loss 1.4501398801803589; generator loss 1.021178960800171\n",
      "training step 43000: discriminator loss 1.6331843137741089; generator loss -0.0674317330121994\n",
      "Current step:43000\n",
      "training step 43100: discriminator loss 1.426619052886963; generator loss -0.004645790904760361\n",
      "training step 43200: discriminator loss 1.3764816522598267; generator loss 1.0564337968826294\n",
      "training step 43300: discriminator loss 1.672704815864563; generator loss 0.1257551610469818\n",
      "training step 43400: discriminator loss 1.387332797050476; generator loss 0.6317356824874878\n",
      "training step 43500: discriminator loss 1.2762887477874756; generator loss 0.2592603862285614\n",
      "training step 43600: discriminator loss 1.5236449241638184; generator loss 0.9100313782691956\n",
      "training step 43700: discriminator loss 1.4390653371810913; generator loss 0.7600976228713989\n",
      "training step 43800: discriminator loss 1.1441559791564941; generator loss 0.6497761011123657\n",
      "training step 43900: discriminator loss 1.4028770923614502; generator loss 0.2991299629211426\n",
      "training step 44000: discriminator loss 3.2582590579986572; generator loss -2.096771717071533\n",
      "Current step:44000\n",
      "training step 44100: discriminator loss 1.0528159141540527; generator loss 1.10200035572052\n",
      "training step 44200: discriminator loss 1.2623589038848877; generator loss 0.5470361709594727\n",
      "training step 44300: discriminator loss 1.521159052848816; generator loss 1.1378843784332275\n",
      "training step 44400: discriminator loss 1.1908618211746216; generator loss 0.7067372798919678\n",
      "training step 44500: discriminator loss 1.1741623878479004; generator loss 0.7303482294082642\n",
      "training step 44600: discriminator loss 1.5644221305847168; generator loss 1.2115108966827393\n",
      "training step 44700: discriminator loss 1.5019735097885132; generator loss 1.2337102890014648\n",
      "training step 44800: discriminator loss 1.584680199623108; generator loss 0.6227666735649109\n",
      "training step 44900: discriminator loss 1.4413726329803467; generator loss -0.01256929337978363\n",
      "training step 45000: discriminator loss 1.471545934677124; generator loss 0.6524382829666138\n",
      "Current step:45000\n",
      "Saving model snapshot\n",
      "training step 45100: discriminator loss 1.5464385747909546; generator loss 1.3829734325408936\n",
      "training step 45200: discriminator loss 1.380447268486023; generator loss 0.8830592632293701\n",
      "training step 45300: discriminator loss 1.533839225769043; generator loss 1.4835619926452637\n",
      "training step 45400: discriminator loss 1.5836515426635742; generator loss -0.1548866629600525\n",
      "training step 45500: discriminator loss 1.2834076881408691; generator loss 0.2657008767127991\n",
      "training step 45600: discriminator loss 1.1919690370559692; generator loss 0.18372130393981934\n",
      "training step 45700: discriminator loss 1.4885542392730713; generator loss 0.6514603495597839\n",
      "training step 45800: discriminator loss 1.7368459701538086; generator loss 1.3709542751312256\n",
      "training step 45900: discriminator loss 1.1933324337005615; generator loss 0.8794762492179871\n",
      "training step 46000: discriminator loss 1.4519143104553223; generator loss 0.7078785300254822\n",
      "Current step:46000\n",
      "training step 46100: discriminator loss 1.2304179668426514; generator loss 0.9012736082077026\n",
      "training step 46200: discriminator loss 1.5006047487258911; generator loss 1.0386765003204346\n",
      "training step 46300: discriminator loss 1.3144139051437378; generator loss 0.44487929344177246\n",
      "training step 46400: discriminator loss 1.684215784072876; generator loss -0.4380384683609009\n",
      "training step 46500: discriminator loss 1.4172849655151367; generator loss 0.6411019563674927\n",
      "training step 46600: discriminator loss 1.2033276557922363; generator loss 0.6654115915298462\n",
      "training step 46700: discriminator loss 1.3573102951049805; generator loss 0.9165127277374268\n",
      "training step 46800: discriminator loss 1.3810069561004639; generator loss 0.19899165630340576\n",
      "training step 46900: discriminator loss 1.5255980491638184; generator loss 1.154831886291504\n",
      "training step 47000: discriminator loss 1.5605063438415527; generator loss -0.031004685908555984\n",
      "Current step:47000\n",
      "training step 47100: discriminator loss 1.092038631439209; generator loss 1.0062475204467773\n",
      "training step 47200: discriminator loss 1.8914737701416016; generator loss -0.5965282917022705\n",
      "training step 47300: discriminator loss 1.5554379224777222; generator loss 1.0614981651306152\n",
      "training step 47400: discriminator loss 1.3433949947357178; generator loss 0.5688137412071228\n",
      "training step 47500: discriminator loss 1.7934980392456055; generator loss 1.7231113910675049\n",
      "Saving model snapshot\n",
      "training step 47600: discriminator loss 1.4673397541046143; generator loss 0.722467303276062\n",
      "training step 47700: discriminator loss 1.568185806274414; generator loss -0.03563213348388672\n",
      "training step 47800: discriminator loss 1.0336354970932007; generator loss 0.5966694355010986\n",
      "training step 47900: discriminator loss 1.2852704524993896; generator loss 0.8649300336837769\n",
      "training step 48000: discriminator loss 1.3009032011032104; generator loss 0.7222182750701904\n",
      "Current step:48000\n",
      "training step 48100: discriminator loss 1.4091607332229614; generator loss 0.1694045066833496\n",
      "training step 48200: discriminator loss 1.7117397785186768; generator loss 1.491305947303772\n",
      "training step 48300: discriminator loss 1.7148211002349854; generator loss -0.4433746635913849\n",
      "training step 48400: discriminator loss 1.3581082820892334; generator loss 1.3050105571746826\n",
      "training step 48500: discriminator loss 1.2397170066833496; generator loss 0.3066348135471344\n",
      "training step 48600: discriminator loss 1.5908172130584717; generator loss 1.278505563735962\n",
      "training step 48700: discriminator loss 1.4429795742034912; generator loss 0.7507153153419495\n",
      "training step 48800: discriminator loss 1.7276465892791748; generator loss 1.1145191192626953\n",
      "training step 48900: discriminator loss 1.6125816106796265; generator loss 0.45281273126602173\n",
      "training step 49000: discriminator loss 1.9778945446014404; generator loss 1.6098828315734863\n",
      "Current step:49000\n",
      "training step 49100: discriminator loss 1.7715519666671753; generator loss 1.169743299484253\n",
      "training step 49200: discriminator loss 1.1690312623977661; generator loss 0.9269730448722839\n",
      "training step 49300: discriminator loss 1.5422978401184082; generator loss 0.9637640118598938\n",
      "training step 49400: discriminator loss 1.4831395149230957; generator loss 0.24923169612884521\n",
      "training step 49500: discriminator loss 1.5192793607711792; generator loss 1.203532338142395\n",
      "training step 49600: discriminator loss 1.3849682807922363; generator loss 0.8960630893707275\n",
      "training step 49700: discriminator loss 1.2932742834091187; generator loss 0.17913830280303955\n",
      "training step 49800: discriminator loss 1.190121054649353; generator loss 0.8137127161026001\n",
      "training step 49900: discriminator loss 1.3146278858184814; generator loss 0.8117080330848694\n",
      "training step 50000: discriminator loss 1.211930513381958; generator loss 0.6302999258041382\n",
      "Current step:50000\n",
      "Saving model snapshot\n",
      "training step 50100: discriminator loss 1.3764070272445679; generator loss 0.2349471151828766\n",
      "training step 50200: discriminator loss 1.7059905529022217; generator loss -0.39468103647232056\n",
      "training step 50300: discriminator loss 1.9097850322723389; generator loss -0.5047788023948669\n",
      "training step 50400: discriminator loss 1.536656379699707; generator loss 0.9287130236625671\n",
      "training step 50500: discriminator loss 1.2918832302093506; generator loss 0.4725540280342102\n",
      "training step 50600: discriminator loss 1.3320972919464111; generator loss 0.7465263605117798\n",
      "training step 50700: discriminator loss 1.6818294525146484; generator loss -0.3583352863788605\n",
      "training step 50800: discriminator loss 1.5372034311294556; generator loss -0.030600305646657944\n",
      "training step 50900: discriminator loss 1.8857685327529907; generator loss -0.17017334699630737\n",
      "training step 51000: discriminator loss 1.5367499589920044; generator loss 0.11820026487112045\n",
      "Current step:51000\n",
      "training step 51100: discriminator loss 1.6181026697158813; generator loss 0.08944478631019592\n",
      "training step 51200: discriminator loss 1.464758038520813; generator loss 1.0492448806762695\n",
      "training step 51300: discriminator loss 1.5047472715377808; generator loss 1.0116958618164062\n",
      "training step 51400: discriminator loss 1.7076172828674316; generator loss -0.15837155282497406\n",
      "training step 51500: discriminator loss 1.6154258251190186; generator loss 1.1840224266052246\n",
      "training step 51600: discriminator loss 1.5588302612304688; generator loss 0.2503425180912018\n",
      "training step 51700: discriminator loss 1.7077161073684692; generator loss -0.04009465128183365\n",
      "training step 51800: discriminator loss 1.2642159461975098; generator loss 0.40652668476104736\n",
      "training step 51900: discriminator loss 1.4182533025741577; generator loss 0.3147883415222168\n",
      "training step 52000: discriminator loss 1.4360308647155762; generator loss 0.2817712128162384\n",
      "Current step:52000\n",
      "training step 52100: discriminator loss 1.5057764053344727; generator loss 0.15930721163749695\n",
      "training step 52200: discriminator loss 1.7940562963485718; generator loss -0.347557008266449\n",
      "training step 52300: discriminator loss 1.5922656059265137; generator loss -0.26476022601127625\n",
      "training step 52400: discriminator loss 1.2476019859313965; generator loss 0.951433539390564\n",
      "training step 52500: discriminator loss 1.6811388731002808; generator loss 1.0670086145401\n",
      "Saving model snapshot\n",
      "training step 52600: discriminator loss 1.5400199890136719; generator loss 0.9628392457962036\n",
      "training step 52700: discriminator loss 1.5346590280532837; generator loss 0.316508024930954\n",
      "training step 52800: discriminator loss 1.5961670875549316; generator loss 0.06235307455062866\n",
      "training step 52900: discriminator loss 1.7729251384735107; generator loss -0.6793323159217834\n",
      "training step 53000: discriminator loss 1.4164879322052002; generator loss 0.5623455047607422\n",
      "Current step:53000\n",
      "training step 53100: discriminator loss 2.212158441543579; generator loss -0.8972066640853882\n",
      "training step 53200: discriminator loss 1.5499837398529053; generator loss 0.34703969955444336\n",
      "training step 53300: discriminator loss 1.6155662536621094; generator loss 0.17684707045555115\n",
      "training step 53400: discriminator loss 1.439976692199707; generator loss 0.20523054897785187\n",
      "training step 53500: discriminator loss 1.6891977787017822; generator loss 1.1747221946716309\n",
      "training step 53600: discriminator loss 1.5867164134979248; generator loss 0.9555908441543579\n",
      "training step 53700: discriminator loss 1.6428101062774658; generator loss -0.045966457575559616\n",
      "training step 53800: discriminator loss 1.389679193496704; generator loss 0.9387275576591492\n",
      "training step 53900: discriminator loss 1.4692959785461426; generator loss 0.772640585899353\n",
      "training step 54000: discriminator loss 1.3343305587768555; generator loss 0.5658187866210938\n",
      "Current step:54000\n",
      "training step 54100: discriminator loss 1.723174810409546; generator loss -0.3177970051765442\n",
      "training step 54200: discriminator loss 1.4921640157699585; generator loss 1.0221288204193115\n",
      "training step 54300: discriminator loss 1.8527277708053589; generator loss -0.6542384028434753\n",
      "training step 54400: discriminator loss 1.327838659286499; generator loss 0.814719557762146\n",
      "training step 54500: discriminator loss 1.394221544265747; generator loss 0.06485721468925476\n",
      "training step 54600: discriminator loss 1.3086941242218018; generator loss -0.03294040262699127\n",
      "training step 54700: discriminator loss 1.2920070886611938; generator loss 0.6800909042358398\n",
      "training step 54800: discriminator loss 1.5901442766189575; generator loss 1.075209617614746\n",
      "training step 54900: discriminator loss 1.5279556512832642; generator loss -0.18603740632534027\n",
      "training step 55000: discriminator loss 1.4365679025650024; generator loss 0.16628104448318481\n",
      "Current step:55000\n",
      "Saving model snapshot\n",
      "training step 55100: discriminator loss 1.3617544174194336; generator loss 1.125178337097168\n",
      "training step 55200: discriminator loss 1.2730236053466797; generator loss 0.741366982460022\n",
      "training step 55300: discriminator loss 1.3396904468536377; generator loss 1.130890130996704\n",
      "training step 55400: discriminator loss 1.4570527076721191; generator loss 0.060884520411491394\n",
      "training step 55500: discriminator loss 1.3167099952697754; generator loss 0.7694059610366821\n",
      "training step 55600: discriminator loss 1.2260648012161255; generator loss 0.7432452440261841\n",
      "training step 55700: discriminator loss 1.519436001777649; generator loss -0.05007634684443474\n",
      "training step 55800: discriminator loss 1.4152472019195557; generator loss 0.8812708258628845\n",
      "training step 55900: discriminator loss 1.3474878072738647; generator loss 0.7754315137863159\n",
      "training step 56000: discriminator loss 1.2537232637405396; generator loss 0.7408323287963867\n",
      "Current step:56000\n",
      "training step 56100: discriminator loss 1.520453691482544; generator loss 0.7871171832084656\n",
      "training step 56200: discriminator loss 1.6637349128723145; generator loss -0.2233332395553589\n",
      "training step 56300: discriminator loss 1.5191106796264648; generator loss 0.3337024450302124\n",
      "training step 56400: discriminator loss 1.4559916257858276; generator loss 0.928369402885437\n",
      "training step 56500: discriminator loss 2.0087249279022217; generator loss -0.7806479930877686\n",
      "training step 56600: discriminator loss 1.4706557989120483; generator loss 0.594829261302948\n",
      "training step 56700: discriminator loss 1.5559742450714111; generator loss -0.0551186203956604\n",
      "training step 56800: discriminator loss 1.7439757585525513; generator loss 0.13569046556949615\n",
      "training step 56900: discriminator loss 1.749005675315857; generator loss -0.5141807794570923\n",
      "training step 57000: discriminator loss 1.4762156009674072; generator loss -0.09876134246587753\n",
      "Current step:57000\n",
      "training step 57100: discriminator loss 1.3681155443191528; generator loss 0.9958099126815796\n",
      "training step 57200: discriminator loss 1.4202983379364014; generator loss 0.35846787691116333\n",
      "training step 57300: discriminator loss 1.7676218748092651; generator loss 0.8554364442825317\n",
      "training step 57400: discriminator loss 1.5182584524154663; generator loss 1.0742013454437256\n",
      "training step 57500: discriminator loss 1.441372036933899; generator loss 0.3450826406478882\n",
      "Saving model snapshot\n",
      "training step 57600: discriminator loss 1.5487537384033203; generator loss 0.8011192083358765\n",
      "training step 57700: discriminator loss 1.4611936807632446; generator loss 0.27222585678100586\n",
      "training step 57800: discriminator loss 1.632202386856079; generator loss 1.401576280593872\n",
      "training step 57900: discriminator loss 1.3462096452713013; generator loss 0.9253005981445312\n",
      "training step 58000: discriminator loss 1.3242466449737549; generator loss 0.876577615737915\n",
      "Current step:58000\n",
      "training step 58100: discriminator loss 1.4298255443572998; generator loss 0.22964170575141907\n",
      "training step 58200: discriminator loss 1.485527753829956; generator loss 0.8699694871902466\n",
      "training step 58300: discriminator loss 1.833054780960083; generator loss -0.375926673412323\n",
      "training step 58400: discriminator loss 1.512322187423706; generator loss 1.1748888492584229\n",
      "training step 58500: discriminator loss 1.8349202871322632; generator loss 1.219261646270752\n",
      "training step 58600: discriminator loss 1.6011981964111328; generator loss 0.01589125394821167\n",
      "training step 58700: discriminator loss 1.3624677658081055; generator loss 0.7686570882797241\n",
      "training step 58800: discriminator loss 1.5218125581741333; generator loss 0.21474577486515045\n",
      "training step 58900: discriminator loss 2.0962753295898438; generator loss -0.9709596633911133\n",
      "training step 59000: discriminator loss 1.6462143659591675; generator loss -0.22435946762561798\n",
      "Current step:59000\n",
      "training step 59100: discriminator loss 1.6020315885543823; generator loss 1.3524267673492432\n",
      "training step 59200: discriminator loss 1.9093589782714844; generator loss 1.83078932762146\n",
      "training step 59300: discriminator loss 1.2033101320266724; generator loss 0.8314125537872314\n",
      "training step 59400: discriminator loss 1.2744662761688232; generator loss 0.4688354730606079\n",
      "training step 59500: discriminator loss 1.2316198348999023; generator loss 0.24673418700695038\n",
      "training step 59600: discriminator loss 1.6331286430358887; generator loss 0.6583481431007385\n",
      "training step 59700: discriminator loss 1.5468800067901611; generator loss 1.2285410165786743\n",
      "training step 59800: discriminator loss 1.6455597877502441; generator loss -0.0784309133887291\n",
      "training step 59900: discriminator loss 1.3880233764648438; generator loss 0.6871745586395264\n",
      "training step 60000: discriminator loss 1.209548830986023; generator loss 0.45443809032440186\n",
      "Current step:60000\n",
      "Saving model snapshot\n",
      "training step 60100: discriminator loss 1.8664753437042236; generator loss -0.5529364347457886\n",
      "training step 60200: discriminator loss 1.387810468673706; generator loss 0.024045448750257492\n",
      "training step 60300: discriminator loss 1.5412898063659668; generator loss 0.08651664853096008\n",
      "training step 60400: discriminator loss 1.8712928295135498; generator loss -0.13777302205562592\n",
      "training step 60500: discriminator loss 1.5829124450683594; generator loss 1.0873407125473022\n",
      "training step 60600: discriminator loss 1.444610357284546; generator loss 0.007874161005020142\n",
      "training step 60700: discriminator loss 1.3021619319915771; generator loss 0.5136576890945435\n",
      "training step 60800: discriminator loss 1.4339526891708374; generator loss 0.7703989744186401\n",
      "training step 60900: discriminator loss 1.6922540664672852; generator loss 1.339285135269165\n",
      "training step 61000: discriminator loss 1.4383020401000977; generator loss 1.0422508716583252\n",
      "Current step:61000\n",
      "training step 61100: discriminator loss 1.4478306770324707; generator loss 0.8170677423477173\n",
      "training step 61200: discriminator loss 1.1721298694610596; generator loss 1.0707868337631226\n",
      "training step 61300: discriminator loss 1.706329584121704; generator loss -0.09150870144367218\n",
      "training step 61400: discriminator loss 1.1887669563293457; generator loss 0.8518455624580383\n",
      "training step 61500: discriminator loss 1.346083402633667; generator loss 1.1367685794830322\n",
      "training step 61600: discriminator loss 1.7886806726455688; generator loss -0.6238531470298767\n",
      "training step 61700: discriminator loss 1.5594617128372192; generator loss 0.13959500193595886\n",
      "training step 61800: discriminator loss 1.405916452407837; generator loss 0.3015219271183014\n",
      "training step 61900: discriminator loss 1.3936340808868408; generator loss 0.41859081387519836\n",
      "training step 62000: discriminator loss 1.431183099746704; generator loss 0.26744189858436584\n",
      "Current step:62000\n",
      "training step 62100: discriminator loss 1.593017339706421; generator loss -0.24442671239376068\n",
      "training step 62200: discriminator loss 1.567521095275879; generator loss 1.2682737112045288\n",
      "training step 62300: discriminator loss 1.3309272527694702; generator loss 0.39360812306404114\n",
      "training step 62400: discriminator loss 1.468979835510254; generator loss -0.035820797085762024\n",
      "training step 62500: discriminator loss 1.760283350944519; generator loss -0.21872776746749878\n",
      "Saving model snapshot\n",
      "training step 62600: discriminator loss 1.563481092453003; generator loss -0.0965503379702568\n",
      "training step 62700: discriminator loss 1.5352354049682617; generator loss 1.2169263362884521\n",
      "training step 62800: discriminator loss 1.4230800867080688; generator loss 0.2035234570503235\n",
      "training step 62900: discriminator loss 1.6225595474243164; generator loss 0.4474946856498718\n",
      "training step 63000: discriminator loss 1.4643659591674805; generator loss 0.05785737931728363\n",
      "Current step:63000\n",
      "training step 63100: discriminator loss 2.997978687286377; generator loss -1.8783520460128784\n",
      "training step 63200: discriminator loss 1.5059385299682617; generator loss 0.33316129446029663\n",
      "training step 63300: discriminator loss 1.6086331605911255; generator loss -0.07769938558340073\n",
      "training step 63400: discriminator loss 1.199357271194458; generator loss 0.6993100643157959\n",
      "training step 63500: discriminator loss 1.5727545022964478; generator loss 1.1057243347167969\n",
      "training step 63600: discriminator loss 1.9446487426757812; generator loss -0.7938723564147949\n",
      "training step 63700: discriminator loss 1.390368103981018; generator loss 0.10610461235046387\n",
      "training step 63800: discriminator loss 1.5866667032241821; generator loss -0.13674411177635193\n",
      "training step 63900: discriminator loss 1.2300485372543335; generator loss 0.4523376524448395\n",
      "training step 64000: discriminator loss 1.3392713069915771; generator loss 0.5998269319534302\n",
      "Current step:64000\n",
      "training step 64100: discriminator loss 1.2835171222686768; generator loss 0.4308501183986664\n",
      "training step 64200: discriminator loss 1.8586053848266602; generator loss -0.5143811702728271\n",
      "training step 64300: discriminator loss 1.5236237049102783; generator loss 1.099194049835205\n",
      "training step 64400: discriminator loss 1.4387943744659424; generator loss 0.8717080950737\n",
      "training step 64500: discriminator loss 1.3648549318313599; generator loss 0.1286313384771347\n",
      "training step 64600: discriminator loss 1.4387474060058594; generator loss 0.43261095881462097\n",
      "training step 64700: discriminator loss 1.408125400543213; generator loss 0.8288877010345459\n",
      "training step 64800: discriminator loss 1.2505372762680054; generator loss 0.4849080741405487\n",
      "training step 64900: discriminator loss 1.6479978561401367; generator loss -0.1938660889863968\n",
      "training step 65000: discriminator loss 1.311431884765625; generator loss 0.3930574655532837\n",
      "Current step:65000\n",
      "Saving model snapshot\n",
      "training step 65100: discriminator loss 1.701566457748413; generator loss -0.3005538582801819\n",
      "training step 65200: discriminator loss 1.506809949874878; generator loss 0.03592162951827049\n",
      "training step 65300: discriminator loss 1.3439573049545288; generator loss 1.2295746803283691\n",
      "training step 65400: discriminator loss 1.474350929260254; generator loss 0.03574178367853165\n",
      "training step 65500: discriminator loss 1.401665449142456; generator loss 1.1836304664611816\n",
      "training step 65600: discriminator loss 1.5419824123382568; generator loss 0.7090113162994385\n",
      "training step 65700: discriminator loss 1.2545826435089111; generator loss 0.8946611881256104\n",
      "training step 65800: discriminator loss 1.5813071727752686; generator loss 1.6094673871994019\n",
      "training step 65900: discriminator loss 1.6368474960327148; generator loss 1.2893584966659546\n",
      "training step 66000: discriminator loss 1.3031461238861084; generator loss 0.7820369005203247\n",
      "Current step:66000\n",
      "training step 66100: discriminator loss 1.3070390224456787; generator loss 0.5799245834350586\n",
      "training step 66200: discriminator loss 1.3938860893249512; generator loss 0.9869372844696045\n",
      "training step 66300: discriminator loss 1.165137767791748; generator loss 0.9651904106140137\n",
      "training step 66400: discriminator loss 1.6033908128738403; generator loss 0.23685351014137268\n",
      "training step 66500: discriminator loss 1.2951005697250366; generator loss 0.8098767399787903\n",
      "training step 66600: discriminator loss 1.1962342262268066; generator loss 0.9587987661361694\n",
      "training step 66700: discriminator loss 1.492360234260559; generator loss 0.7985655069351196\n",
      "training step 66800: discriminator loss 1.7078937292099; generator loss -0.423667848110199\n",
      "training step 66900: discriminator loss 1.4091126918792725; generator loss 0.7294849157333374\n",
      "training step 67000: discriminator loss 1.4904099702835083; generator loss 0.9372743368148804\n",
      "Current step:67000\n",
      "training step 67100: discriminator loss 1.335615873336792; generator loss 0.5877038240432739\n",
      "training step 67200: discriminator loss 1.3941900730133057; generator loss 0.7407487034797668\n",
      "training step 67300: discriminator loss 1.4862321615219116; generator loss 1.0929187536239624\n",
      "training step 67400: discriminator loss 1.4949114322662354; generator loss 0.9786931872367859\n",
      "training step 67500: discriminator loss 2.031318187713623; generator loss 1.847801923751831\n",
      "Saving model snapshot\n",
      "training step 67600: discriminator loss 1.577397108078003; generator loss 0.3133416175842285\n",
      "training step 67700: discriminator loss 1.3749995231628418; generator loss 1.0951905250549316\n",
      "training step 67800: discriminator loss 1.1748461723327637; generator loss 0.4998400807380676\n",
      "training step 67900: discriminator loss 1.4971823692321777; generator loss 0.8305152058601379\n",
      "training step 68000: discriminator loss 1.8111157417297363; generator loss 1.3978064060211182\n",
      "Current step:68000\n",
      "training step 68100: discriminator loss 1.1564252376556396; generator loss 0.7829023599624634\n",
      "training step 68200: discriminator loss 1.3288381099700928; generator loss 0.3902716338634491\n",
      "training step 68300: discriminator loss 1.5168284177780151; generator loss 1.0608007907867432\n",
      "training step 68400: discriminator loss 1.4072149991989136; generator loss 0.3094567358493805\n",
      "training step 68500: discriminator loss 1.20949125289917; generator loss 0.7782819271087646\n",
      "training step 68600: discriminator loss 1.310232400894165; generator loss 0.5114126205444336\n",
      "training step 68700: discriminator loss 1.514719009399414; generator loss -0.005890056490898132\n",
      "training step 68800: discriminator loss 1.5815997123718262; generator loss 1.0790709257125854\n",
      "training step 68900: discriminator loss 1.48860764503479; generator loss 1.1001412868499756\n",
      "training step 69000: discriminator loss 1.5298911333084106; generator loss 1.1275861263275146\n",
      "Current step:69000\n",
      "training step 69100: discriminator loss 1.23380708694458; generator loss 1.30031156539917\n",
      "training step 69200: discriminator loss 1.3267111778259277; generator loss 0.2917059361934662\n",
      "training step 69300: discriminator loss 1.2788900136947632; generator loss 1.130742073059082\n",
      "training step 69400: discriminator loss 1.4366979598999023; generator loss 0.3815014362335205\n",
      "training step 69500: discriminator loss 1.290366291999817; generator loss 1.0247398614883423\n",
      "training step 69600: discriminator loss 1.343855857849121; generator loss 0.2608868181705475\n",
      "training step 69700: discriminator loss 1.3236279487609863; generator loss 1.298732876777649\n",
      "training step 69800: discriminator loss 1.3808510303497314; generator loss 0.6072914600372314\n",
      "training step 69900: discriminator loss 1.3928112983703613; generator loss 0.7280700206756592\n",
      "training step 70000: discriminator loss 1.3096702098846436; generator loss 0.8734468221664429\n",
      "Current step:70000\n",
      "Saving model snapshot\n",
      "training step 70100: discriminator loss 1.6963615417480469; generator loss 1.46608304977417\n",
      "training step 70200: discriminator loss 1.7036051750183105; generator loss -0.3904986083507538\n",
      "training step 70300: discriminator loss 1.7734074592590332; generator loss 1.1533253192901611\n",
      "training step 70400: discriminator loss 1.4877208471298218; generator loss 0.7745838165283203\n",
      "training step 70500: discriminator loss 1.5444753170013428; generator loss -0.2753276824951172\n",
      "training step 70600: discriminator loss 1.3793927431106567; generator loss 0.378385066986084\n",
      "training step 70700: discriminator loss 1.3464573621749878; generator loss 0.6980280876159668\n",
      "training step 70800: discriminator loss 1.8305022716522217; generator loss -0.4446832537651062\n",
      "training step 70900: discriminator loss 1.3879530429840088; generator loss 1.2136828899383545\n",
      "training step 71000: discriminator loss 1.2226041555404663; generator loss 0.5300724506378174\n",
      "Current step:71000\n",
      "training step 71100: discriminator loss 1.2794222831726074; generator loss 0.1281629204750061\n",
      "training step 71200: discriminator loss 1.5767689943313599; generator loss 1.6729665994644165\n",
      "training step 71300: discriminator loss 1.496699333190918; generator loss 0.8531016111373901\n",
      "training step 71400: discriminator loss 1.787993311882019; generator loss -0.48227059841156006\n",
      "training step 71500: discriminator loss 1.3656866550445557; generator loss 0.9828105568885803\n",
      "training step 71600: discriminator loss 1.3558411598205566; generator loss 0.9034749269485474\n",
      "training step 71700: discriminator loss 0.9855226874351501; generator loss 0.8603331446647644\n",
      "training step 71800: discriminator loss 1.34127938747406; generator loss 0.06826937198638916\n",
      "training step 71900: discriminator loss 1.3551414012908936; generator loss 0.702836275100708\n",
      "training step 72000: discriminator loss 1.299868106842041; generator loss 0.43648073077201843\n",
      "Current step:72000\n",
      "training step 72100: discriminator loss 1.3162099123001099; generator loss 0.42261332273483276\n",
      "training step 72200: discriminator loss 1.587193489074707; generator loss -0.15078192949295044\n",
      "training step 72300: discriminator loss 1.3695993423461914; generator loss 0.21020084619522095\n",
      "training step 72400: discriminator loss 1.3288477659225464; generator loss 0.8897747993469238\n",
      "training step 72500: discriminator loss 2.512969493865967; generator loss -1.4118940830230713\n",
      "Saving model snapshot\n",
      "training step 72600: discriminator loss 1.232015609741211; generator loss 0.5178118348121643\n",
      "training step 72700: discriminator loss 1.4748951196670532; generator loss 0.8337724804878235\n",
      "training step 72800: discriminator loss 1.5029592514038086; generator loss 0.4369846284389496\n",
      "training step 72900: discriminator loss 1.2043815851211548; generator loss 0.35745495557785034\n",
      "training step 73000: discriminator loss 1.523935317993164; generator loss 0.06593766808509827\n",
      "Current step:73000\n",
      "training step 73100: discriminator loss 1.1783947944641113; generator loss 0.932590663433075\n",
      "training step 73200: discriminator loss 1.3376303911209106; generator loss 0.9693339467048645\n",
      "training step 73300: discriminator loss 1.219151496887207; generator loss 0.49687981605529785\n",
      "training step 73400: discriminator loss 1.7430070638656616; generator loss -0.43185320496559143\n",
      "training step 73500: discriminator loss 1.7383975982666016; generator loss 1.3637614250183105\n",
      "training step 73600: discriminator loss 1.2493764162063599; generator loss 0.4098944067955017\n",
      "training step 73700: discriminator loss 1.1844043731689453; generator loss 0.5113806128501892\n",
      "training step 73800: discriminator loss 1.3311234712600708; generator loss 1.1932820081710815\n",
      "training step 73900: discriminator loss 1.2527730464935303; generator loss 0.9562726616859436\n",
      "training step 74000: discriminator loss 1.5411306619644165; generator loss 0.5790299773216248\n",
      "Current step:74000\n",
      "training step 74100: discriminator loss 1.740638017654419; generator loss 0.08743955940008163\n",
      "training step 74200: discriminator loss 1.325704574584961; generator loss 0.6415374875068665\n",
      "training step 74300: discriminator loss 1.227735996246338; generator loss 0.5941426157951355\n",
      "training step 74400: discriminator loss 1.618056297302246; generator loss 0.9458889961242676\n",
      "training step 74500: discriminator loss 1.4413152933120728; generator loss -0.054222144186496735\n",
      "training step 74600: discriminator loss 1.4025819301605225; generator loss 0.4830368161201477\n",
      "training step 74700: discriminator loss 1.286341667175293; generator loss 0.2503003776073456\n",
      "training step 74800: discriminator loss 1.6060084104537964; generator loss -0.13371682167053223\n",
      "training step 74900: discriminator loss 1.4476627111434937; generator loss 0.06707931309938431\n",
      "training step 75000: discriminator loss 1.261977195739746; generator loss 0.570000171661377\n",
      "Current step:75000\n",
      "Saving model snapshot\n",
      "training step 75100: discriminator loss 1.5759152173995972; generator loss 0.010368123650550842\n",
      "training step 75200: discriminator loss 1.5769468545913696; generator loss -0.16306965053081512\n",
      "training step 75300: discriminator loss 1.3993637561798096; generator loss 0.5603489875793457\n",
      "training step 75400: discriminator loss 1.2834628820419312; generator loss 1.023996114730835\n",
      "training step 75500: discriminator loss 1.762115478515625; generator loss 1.44752836227417\n",
      "training step 75600: discriminator loss 1.6508227586746216; generator loss -0.4066162407398224\n",
      "training step 75700: discriminator loss 2.1374518871307373; generator loss -0.6862027645111084\n",
      "training step 75800: discriminator loss 1.5461139678955078; generator loss 1.0609211921691895\n",
      "training step 75900: discriminator loss 1.6177685260772705; generator loss -0.23567238450050354\n",
      "training step 76000: discriminator loss 1.2192635536193848; generator loss 0.23752839863300323\n",
      "Current step:76000\n",
      "training step 76100: discriminator loss 1.3399271965026855; generator loss 0.32773035764694214\n",
      "training step 76200: discriminator loss 1.1541345119476318; generator loss 0.408977210521698\n",
      "training step 76300: discriminator loss 1.4644958972930908; generator loss 0.23757921159267426\n",
      "training step 76400: discriminator loss 1.3140020370483398; generator loss 0.7644426822662354\n",
      "training step 76500: discriminator loss 1.2975051403045654; generator loss 0.7937002778053284\n",
      "training step 76600: discriminator loss 1.3200465440750122; generator loss 0.2464071810245514\n",
      "training step 76700: discriminator loss 1.2521982192993164; generator loss 0.5204417705535889\n",
      "training step 76800: discriminator loss 1.4582741260528564; generator loss 0.6497001647949219\n",
      "training step 76900: discriminator loss 1.5240405797958374; generator loss 1.1662572622299194\n",
      "training step 77000: discriminator loss 1.5758299827575684; generator loss -0.022664695978164673\n",
      "Current step:77000\n",
      "training step 77100: discriminator loss 1.5765202045440674; generator loss 1.4274849891662598\n",
      "training step 77200: discriminator loss 1.2030378580093384; generator loss 1.000583529472351\n",
      "training step 77300: discriminator loss 1.7555263042449951; generator loss -0.40339866280555725\n",
      "training step 77400: discriminator loss 1.3692070245742798; generator loss 0.9862812757492065\n",
      "training step 77500: discriminator loss 1.4833991527557373; generator loss 0.16316711902618408\n",
      "Saving model snapshot\n",
      "training step 77600: discriminator loss 1.4179494380950928; generator loss 0.2916550040245056\n",
      "training step 77700: discriminator loss 1.6363657712936401; generator loss 1.232445240020752\n",
      "training step 77800: discriminator loss 1.476957082748413; generator loss -0.1229458674788475\n",
      "training step 77900: discriminator loss 1.4189890623092651; generator loss 1.138742208480835\n",
      "training step 78000: discriminator loss 1.216340184211731; generator loss 0.3198148012161255\n",
      "Current step:78000\n",
      "training step 78100: discriminator loss 1.399825930595398; generator loss 0.07200250029563904\n",
      "training step 78200: discriminator loss 1.3795692920684814; generator loss 0.6317030191421509\n",
      "training step 78300: discriminator loss 1.674604892730713; generator loss 1.0324749946594238\n",
      "training step 78400: discriminator loss 1.787999153137207; generator loss -0.5356491208076477\n",
      "training step 78500: discriminator loss 1.2505439519882202; generator loss 0.91841721534729\n",
      "training step 78600: discriminator loss 1.2488714456558228; generator loss 1.0535846948623657\n",
      "training step 78700: discriminator loss 1.71559476852417; generator loss -0.22708064317703247\n",
      "training step 78800: discriminator loss 1.3200689554214478; generator loss 0.9602013826370239\n",
      "training step 78900: discriminator loss 1.2260451316833496; generator loss 1.0893967151641846\n",
      "training step 79000: discriminator loss 1.6484274864196777; generator loss -0.1993636041879654\n",
      "Current step:79000\n",
      "training step 79100: discriminator loss 1.6125473976135254; generator loss -0.21934938430786133\n",
      "training step 79200: discriminator loss 1.4357364177703857; generator loss 1.191191554069519\n",
      "training step 79300: discriminator loss 1.2793734073638916; generator loss 0.9770642518997192\n",
      "training step 79400: discriminator loss 1.5339062213897705; generator loss -0.19810757040977478\n",
      "training step 79500: discriminator loss 1.4609142541885376; generator loss 1.1467365026474\n",
      "training step 79600: discriminator loss 1.5144134759902954; generator loss 1.453936219215393\n",
      "training step 79700: discriminator loss 1.6214096546173096; generator loss -0.26157888770103455\n",
      "training step 79800: discriminator loss 1.5137615203857422; generator loss -0.15615913271903992\n",
      "training step 79900: discriminator loss 2.326141357421875; generator loss -1.1513252258300781\n",
      "training step 80000: discriminator loss 1.4467146396636963; generator loss 0.12912200391292572\n",
      "Current step:80000\n",
      "Saving model snapshot\n",
      "training step 80100: discriminator loss 2.230691909790039; generator loss -0.9000880718231201\n",
      "training step 80200: discriminator loss 1.410051941871643; generator loss 1.325740098953247\n",
      "training step 80300: discriminator loss 1.2079997062683105; generator loss 0.25835609436035156\n",
      "training step 80400: discriminator loss 1.392460584640503; generator loss 0.41239386796951294\n",
      "training step 80500: discriminator loss 1.3573517799377441; generator loss 0.09198271483182907\n",
      "training step 80600: discriminator loss 1.3531222343444824; generator loss 1.068749189376831\n",
      "training step 80700: discriminator loss 1.7587727308273315; generator loss -0.2606550455093384\n",
      "training step 80800: discriminator loss 1.5399854183197021; generator loss 0.2552935481071472\n",
      "training step 80900: discriminator loss 1.3437825441360474; generator loss 0.05842535197734833\n",
      "training step 81000: discriminator loss 1.46515953540802; generator loss 0.7067431211471558\n",
      "Current step:81000\n",
      "training step 81100: discriminator loss 1.4199250936508179; generator loss -0.031578391790390015\n",
      "training step 81200: discriminator loss 1.2445168495178223; generator loss 1.3023160696029663\n",
      "training step 81300: discriminator loss 1.5005741119384766; generator loss 1.0678305625915527\n",
      "training step 81400: discriminator loss 1.3495280742645264; generator loss 1.0224554538726807\n",
      "training step 81500: discriminator loss 1.6570228338241577; generator loss -0.2273966521024704\n",
      "training step 81600: discriminator loss 1.3063554763793945; generator loss 1.2827174663543701\n",
      "training step 81700: discriminator loss 1.9379284381866455; generator loss -0.6772834062576294\n",
      "training step 81800: discriminator loss 1.2205784320831299; generator loss 0.45428550243377686\n",
      "training step 81900: discriminator loss 1.3338627815246582; generator loss 0.2589607536792755\n",
      "training step 82000: discriminator loss 1.2374083995819092; generator loss 0.6409808397293091\n",
      "Current step:82000\n",
      "training step 82100: discriminator loss 1.509479284286499; generator loss -0.12253281474113464\n",
      "training step 82200: discriminator loss 1.3248744010925293; generator loss 0.7893129587173462\n",
      "training step 82300: discriminator loss 1.5132019519805908; generator loss 1.004887342453003\n",
      "training step 82400: discriminator loss 1.315481185913086; generator loss 0.8430787324905396\n",
      "training step 82500: discriminator loss 1.7615975141525269; generator loss 1.3285728693008423\n",
      "Saving model snapshot\n",
      "training step 82600: discriminator loss 1.2474279403686523; generator loss 0.18687967956066132\n",
      "training step 82700: discriminator loss 1.2599198818206787; generator loss 0.5927258729934692\n",
      "training step 82800: discriminator loss 1.6385995149612427; generator loss 1.4664183855056763\n",
      "training step 82900: discriminator loss 1.335756540298462; generator loss 1.0362510681152344\n",
      "training step 83000: discriminator loss 1.1694893836975098; generator loss 0.7687788009643555\n",
      "Current step:83000\n",
      "training step 83100: discriminator loss 1.8483555316925049; generator loss -0.46450191736221313\n",
      "training step 83200: discriminator loss 1.2164644002914429; generator loss 0.8919859528541565\n",
      "training step 83300: discriminator loss 1.60370934009552; generator loss 1.308090329170227\n",
      "training step 83400: discriminator loss 1.6760865449905396; generator loss 1.238698124885559\n",
      "training step 83500: discriminator loss 1.5030287504196167; generator loss 0.05829237401485443\n",
      "training step 83600: discriminator loss 1.377855896949768; generator loss 0.4034236669540405\n",
      "training step 83700: discriminator loss 1.2748286724090576; generator loss 0.41541588306427\n",
      "training step 83800: discriminator loss 1.3947181701660156; generator loss 0.44361597299575806\n",
      "training step 83900: discriminator loss 1.4743322134017944; generator loss 0.032602764666080475\n",
      "training step 84000: discriminator loss 1.424263596534729; generator loss 1.284895420074463\n",
      "Current step:84000\n",
      "training step 84100: discriminator loss 1.4762930870056152; generator loss 0.5607571601867676\n",
      "training step 84200: discriminator loss 1.183138132095337; generator loss 0.6122192740440369\n",
      "training step 84300: discriminator loss 1.1388683319091797; generator loss 1.0416090488433838\n",
      "training step 84400: discriminator loss 1.5431544780731201; generator loss 1.186212182044983\n",
      "training step 84500: discriminator loss 1.3382319211959839; generator loss 1.2142137289047241\n",
      "training step 84600: discriminator loss 1.6168478727340698; generator loss 1.3413009643554688\n",
      "training step 84700: discriminator loss 1.730790615081787; generator loss -0.4208354353904724\n",
      "training step 84800: discriminator loss 1.4089514017105103; generator loss 0.3052414655685425\n",
      "training step 84900: discriminator loss 1.1870659589767456; generator loss 1.2654045820236206\n",
      "training step 85000: discriminator loss 1.474325180053711; generator loss 1.113377332687378\n",
      "Current step:85000\n",
      "Saving model snapshot\n",
      "training step 85100: discriminator loss 1.3608978986740112; generator loss 0.934096097946167\n",
      "training step 85200: discriminator loss 1.3441458940505981; generator loss 1.1387619972229004\n",
      "training step 85300: discriminator loss 1.1668858528137207; generator loss 0.48032620549201965\n",
      "training step 85400: discriminator loss 1.3773781061172485; generator loss 1.0072754621505737\n",
      "training step 85500: discriminator loss 1.8686593770980835; generator loss 1.836693525314331\n",
      "training step 85600: discriminator loss 1.0835344791412354; generator loss 0.36817851662635803\n",
      "training step 85700: discriminator loss 1.3519182205200195; generator loss -0.07051041722297668\n",
      "training step 85800: discriminator loss 1.6410446166992188; generator loss 1.2389076948165894\n",
      "training step 85900: discriminator loss 1.507486343383789; generator loss 1.5877784490585327\n",
      "training step 86000: discriminator loss 1.382305383682251; generator loss 0.6997270584106445\n",
      "Current step:86000\n",
      "training step 86100: discriminator loss 1.6759181022644043; generator loss -0.12579438090324402\n",
      "training step 86200: discriminator loss 1.6737666130065918; generator loss -0.33819806575775146\n",
      "training step 86300: discriminator loss 1.4725537300109863; generator loss -0.17324301600456238\n",
      "training step 86400: discriminator loss 1.4933580160140991; generator loss 0.9853262901306152\n",
      "training step 86500: discriminator loss 1.3977049589157104; generator loss 1.0331594944000244\n",
      "training step 86600: discriminator loss 1.9376776218414307; generator loss -0.7542933225631714\n",
      "training step 86700: discriminator loss 1.2286752462387085; generator loss 1.1848876476287842\n",
      "training step 86800: discriminator loss 1.5789650678634644; generator loss -0.0011633262038230896\n",
      "training step 86900: discriminator loss 1.4555344581604004; generator loss 0.45711296796798706\n",
      "training step 87000: discriminator loss 1.254760980606079; generator loss 1.309335708618164\n",
      "Current step:87000\n",
      "training step 87100: discriminator loss 2.509265184402466; generator loss -1.3784351348876953\n",
      "training step 87200: discriminator loss 1.3479681015014648; generator loss 0.8458777666091919\n",
      "training step 87300: discriminator loss 1.3457213640213013; generator loss 1.141287088394165\n",
      "training step 87400: discriminator loss 1.0746524333953857; generator loss 0.6167489290237427\n",
      "training step 87500: discriminator loss 1.8676650524139404; generator loss 1.7129178047180176\n",
      "Saving model snapshot\n",
      "training step 87600: discriminator loss 1.4795634746551514; generator loss 0.12657836079597473\n",
      "training step 87700: discriminator loss 1.1976478099822998; generator loss 0.29409945011138916\n",
      "training step 87800: discriminator loss 1.097111701965332; generator loss 0.7839334011077881\n",
      "training step 87900: discriminator loss 1.842413067817688; generator loss -0.6798900365829468\n",
      "training step 88000: discriminator loss 1.1106023788452148; generator loss 0.9460889101028442\n",
      "Current step:88000\n",
      "training step 88100: discriminator loss 1.4529898166656494; generator loss -0.22897301614284515\n",
      "training step 88200: discriminator loss 1.529280662536621; generator loss 1.3923258781433105\n",
      "training step 88300: discriminator loss 1.218052864074707; generator loss 1.194838523864746\n",
      "training step 88400: discriminator loss 1.232865333557129; generator loss 0.2844496965408325\n",
      "training step 88500: discriminator loss 1.6161067485809326; generator loss -0.3519477844238281\n",
      "training step 88600: discriminator loss 1.2251627445220947; generator loss 1.0972681045532227\n",
      "training step 88700: discriminator loss 1.4703376293182373; generator loss 0.9777412414550781\n",
      "training step 88800: discriminator loss 1.5959638357162476; generator loss 1.3304691314697266\n",
      "training step 88900: discriminator loss 1.2222018241882324; generator loss 0.9412062764167786\n",
      "training step 89000: discriminator loss 1.209243893623352; generator loss 0.6745501756668091\n",
      "Current step:89000\n",
      "training step 89100: discriminator loss 1.152744174003601; generator loss 0.3865131139755249\n",
      "training step 89200: discriminator loss 1.0841987133026123; generator loss 0.9034087657928467\n",
      "training step 89300: discriminator loss 1.0709547996520996; generator loss 1.1009159088134766\n",
      "training step 89400: discriminator loss 1.2508342266082764; generator loss 0.5132148861885071\n",
      "training step 89500: discriminator loss 1.4770427942276; generator loss 0.7708448767662048\n",
      "training step 89600: discriminator loss 1.1754854917526245; generator loss 0.6643301248550415\n",
      "training step 89700: discriminator loss 1.309875249862671; generator loss 0.13496270775794983\n",
      "training step 89800: discriminator loss 2.132737636566162; generator loss -0.8332220315933228\n",
      "training step 89900: discriminator loss 1.386413812637329; generator loss 1.1819106340408325\n",
      "training step 90000: discriminator loss 1.3704237937927246; generator loss 0.6275806427001953\n",
      "Current step:90000\n",
      "Saving model snapshot\n",
      "training step 90100: discriminator loss 1.3651094436645508; generator loss 0.3163527250289917\n",
      "training step 90200: discriminator loss 1.5142790079116821; generator loss 0.9143842458724976\n",
      "training step 90300: discriminator loss 1.1223657131195068; generator loss 1.3355472087860107\n",
      "training step 90400: discriminator loss 1.1163848638534546; generator loss 0.8630588054656982\n",
      "training step 90500: discriminator loss 1.2453150749206543; generator loss 0.2823776602745056\n",
      "training step 90600: discriminator loss 1.1632168292999268; generator loss 0.45817065238952637\n",
      "training step 90700: discriminator loss 1.4716782569885254; generator loss 0.8275036811828613\n",
      "training step 90800: discriminator loss 2.0319924354553223; generator loss -0.6246613264083862\n",
      "training step 90900: discriminator loss 1.280774712562561; generator loss 0.9871855974197388\n",
      "training step 91000: discriminator loss 1.3761643171310425; generator loss 0.11197321861982346\n",
      "Current step:91000\n",
      "training step 91100: discriminator loss 1.4053866863250732; generator loss 0.002904348075389862\n",
      "training step 91200: discriminator loss 1.512534737586975; generator loss 1.3300758600234985\n",
      "training step 91300: discriminator loss 1.524466872215271; generator loss 0.8799689412117004\n",
      "training step 91400: discriminator loss 1.2420008182525635; generator loss 0.7355494499206543\n",
      "training step 91500: discriminator loss 1.4776302576065063; generator loss 1.4848127365112305\n",
      "training step 91600: discriminator loss 1.4400877952575684; generator loss 0.3658957779407501\n",
      "training step 91700: discriminator loss 1.2293884754180908; generator loss 0.4415181875228882\n",
      "training step 91800: discriminator loss 1.1967997550964355; generator loss 0.6579524278640747\n",
      "training step 91900: discriminator loss 1.444557785987854; generator loss 0.05383095145225525\n",
      "training step 92000: discriminator loss 1.3078455924987793; generator loss 0.6769754886627197\n",
      "Current step:92000\n",
      "training step 92100: discriminator loss 1.6879596710205078; generator loss 1.6527318954467773\n",
      "training step 92200: discriminator loss 1.4727522134780884; generator loss 1.2594237327575684\n",
      "training step 92300: discriminator loss 1.8169915676116943; generator loss -0.5171428322792053\n",
      "training step 92400: discriminator loss 1.0692487955093384; generator loss 1.06022047996521\n",
      "training step 92500: discriminator loss 1.6149489879608154; generator loss 1.4174420833587646\n",
      "Saving model snapshot\n",
      "training step 92600: discriminator loss 1.6268984079360962; generator loss 1.7227566242218018\n",
      "training step 92700: discriminator loss 1.1400541067123413; generator loss 1.305001974105835\n",
      "training step 92800: discriminator loss 1.3526496887207031; generator loss 0.17093351483345032\n",
      "training step 92900: discriminator loss 1.3972095251083374; generator loss 0.9648624062538147\n",
      "training step 93000: discriminator loss 1.3650422096252441; generator loss 0.4732707738876343\n",
      "Current step:93000\n",
      "training step 93100: discriminator loss 1.5028419494628906; generator loss -0.1604735255241394\n",
      "training step 93200: discriminator loss 1.5255802869796753; generator loss 0.4365391433238983\n",
      "training step 93300: discriminator loss 1.7253252267837524; generator loss 1.1398184299468994\n",
      "training step 93400: discriminator loss 1.799784779548645; generator loss -0.39069727063179016\n",
      "training step 93500: discriminator loss 1.56992506980896; generator loss -0.22291341423988342\n",
      "training step 93600: discriminator loss 1.1791362762451172; generator loss 0.2188025712966919\n",
      "training step 93700: discriminator loss 1.1058201789855957; generator loss 1.1709158420562744\n",
      "training step 93800: discriminator loss 1.4758021831512451; generator loss 0.0635581836104393\n",
      "training step 93900: discriminator loss 1.1073325872421265; generator loss 0.550099790096283\n",
      "training step 94000: discriminator loss 1.3304388523101807; generator loss 0.3519987463951111\n",
      "Current step:94000\n",
      "training step 94100: discriminator loss 1.4390687942504883; generator loss 1.1798603534698486\n",
      "training step 94200: discriminator loss 1.2347965240478516; generator loss 0.3975309133529663\n",
      "training step 94300: discriminator loss 1.8235092163085938; generator loss -0.5807040929794312\n",
      "training step 94400: discriminator loss 1.9602335691452026; generator loss -0.6570507287979126\n",
      "training step 94500: discriminator loss 1.177724838256836; generator loss 0.8658551573753357\n",
      "training step 94600: discriminator loss 1.3895668983459473; generator loss -0.05244043841958046\n",
      "training step 94700: discriminator loss 1.1606192588806152; generator loss 1.3191055059432983\n",
      "training step 94800: discriminator loss 1.4512810707092285; generator loss 0.4427887499332428\n",
      "training step 94900: discriminator loss 1.022544503211975; generator loss 0.8571771383285522\n",
      "training step 95000: discriminator loss 1.4020791053771973; generator loss 0.2486385703086853\n",
      "Current step:95000\n",
      "Saving model snapshot\n",
      "training step 95100: discriminator loss 1.1796174049377441; generator loss 0.9102879762649536\n",
      "training step 95200: discriminator loss 1.4148375988006592; generator loss 0.9952905178070068\n",
      "training step 95300: discriminator loss 1.3712427616119385; generator loss 1.3045402765274048\n",
      "training step 95400: discriminator loss 1.1354984045028687; generator loss 1.0022698640823364\n",
      "training step 95500: discriminator loss 1.1733299493789673; generator loss 0.6915744543075562\n",
      "training step 95600: discriminator loss 1.1199201345443726; generator loss 0.7057998776435852\n",
      "training step 95700: discriminator loss 1.2729758024215698; generator loss 0.8124493360519409\n",
      "training step 95800: discriminator loss 1.4356836080551147; generator loss 1.4950087070465088\n",
      "training step 95900: discriminator loss 1.2527124881744385; generator loss 0.56133633852005\n",
      "training step 96000: discriminator loss 1.2546054124832153; generator loss 0.9683166742324829\n",
      "Current step:96000\n",
      "training step 96100: discriminator loss 1.6243376731872559; generator loss -0.47536391019821167\n",
      "training step 96200: discriminator loss 1.3518414497375488; generator loss 0.19111965596675873\n",
      "training step 96300: discriminator loss 1.21028733253479; generator loss 0.4528404474258423\n",
      "training step 96400: discriminator loss 1.3818156719207764; generator loss -0.029435165226459503\n",
      "training step 96500: discriminator loss 1.400465726852417; generator loss -0.05146859213709831\n",
      "training step 96600: discriminator loss 1.681758999824524; generator loss 1.6766738891601562\n",
      "training step 96700: discriminator loss 1.220931887626648; generator loss 1.0058186054229736\n",
      "training step 96800: discriminator loss 1.2808197736740112; generator loss 0.29520511627197266\n",
      "training step 96900: discriminator loss 1.0496163368225098; generator loss 1.066136360168457\n",
      "training step 97000: discriminator loss 1.4209342002868652; generator loss 1.5573451519012451\n",
      "Current step:97000\n",
      "training step 97100: discriminator loss 1.423415184020996; generator loss 0.14190611243247986\n",
      "training step 97200: discriminator loss 1.2551875114440918; generator loss 0.9866999387741089\n",
      "training step 97300: discriminator loss 1.4120652675628662; generator loss 0.23551172018051147\n",
      "training step 97400: discriminator loss 1.7320815324783325; generator loss -0.43284526467323303\n",
      "training step 97500: discriminator loss 1.2157764434814453; generator loss 1.0561113357543945\n",
      "Saving model snapshot\n",
      "training step 97600: discriminator loss 1.1554510593414307; generator loss 0.3757816553115845\n",
      "training step 97700: discriminator loss 1.2644505500793457; generator loss 0.9952343702316284\n",
      "training step 97800: discriminator loss 1.2391525506973267; generator loss 0.27301695942878723\n",
      "training step 97900: discriminator loss 1.2099745273590088; generator loss 0.25988346338272095\n",
      "training step 98000: discriminator loss 0.950973391532898; generator loss 1.0814406871795654\n",
      "Current step:98000\n",
      "training step 98100: discriminator loss 1.149207592010498; generator loss 0.4211508631706238\n",
      "training step 98200: discriminator loss 1.2416859865188599; generator loss 1.2583200931549072\n",
      "training step 98300: discriminator loss 1.4783213138580322; generator loss -0.01782156340777874\n",
      "training step 98400: discriminator loss 1.1388378143310547; generator loss 0.33548200130462646\n",
      "training step 98500: discriminator loss 1.4925624132156372; generator loss 1.186036229133606\n",
      "training step 98600: discriminator loss 1.3991491794586182; generator loss 0.8600819706916809\n",
      "training step 98700: discriminator loss 1.340766191482544; generator loss 0.783072829246521\n",
      "training step 98800: discriminator loss 1.5578621625900269; generator loss -0.3143615126609802\n",
      "training step 98900: discriminator loss 1.6240137815475464; generator loss -0.3298472762107849\n",
      "training step 99000: discriminator loss 1.207993745803833; generator loss 1.0494152307510376\n",
      "Current step:99000\n",
      "training step 99100: discriminator loss 1.7953524589538574; generator loss -0.3885236978530884\n",
      "training step 99200: discriminator loss 1.6757220029830933; generator loss -0.21157637238502502\n",
      "training step 99300: discriminator loss 1.4918674230575562; generator loss -0.15722578763961792\n",
      "training step 99400: discriminator loss 1.3724331855773926; generator loss 0.009341927245259285\n",
      "training step 99500: discriminator loss 1.081058144569397; generator loss 0.5721700191497803\n",
      "training step 99600: discriminator loss 1.1712931394577026; generator loss 0.14192698895931244\n",
      "training step 99700: discriminator loss 1.6050636768341064; generator loss 1.2335140705108643\n",
      "training step 99800: discriminator loss 0.993060827255249; generator loss 0.8312010765075684\n",
      "training step 99900: discriminator loss 1.2275550365447998; generator loss 0.7633470892906189\n",
      "training step 100000: discriminator loss 1.4256465435028076; generator loss 0.04999399930238724\n",
      "Current step:100000\n",
      "Saving model snapshot\n",
      "End of training; saving models\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "for _, (batch_real_images) in enumerate(dataset):\n",
    "\n",
    "  # construct random normal z input to feed into generator\n",
    "  input_z = tf.random_normal(shape=(BATCH_SIZE, Z_SIZE), dtype='float32')\n",
    "\n",
    "  with tf.contrib.summary.record_summaries_every_n_global_steps(SUMMARY_PER_N_STEPS):\n",
    "\n",
    "      # define gradient tapes to start recording computation operations\n",
    "      with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
    "          # run generator net with random normal z input to generate image batch\n",
    "          g_fake_images = g_net(input_z, is_training=True)  \n",
    "          # run discriminator net with real input images\n",
    "          d_logits_real = d_net(batch_real_images, is_training=True)\n",
    "          # run discriminator net with generated fake images\n",
    "          d_logits_fake = d_net(g_fake_images, is_training=True)\n",
    "          # compute generator loss by feeding back the discriminator logits output\n",
    "          g_loss = g_net.compute_loss(d_logits_fake)\n",
    "          # compute discriminator hinge loss\n",
    "          d_loss = d_net.compute_loss(d_logits_real, d_logits_fake)\n",
    "\n",
    "      # write losses to tensorboard as scalars & generated images\n",
    "      tf.contrib.summary.scalar('generator_loss', g_loss)\n",
    "      tf.contrib.summary.scalar('discriminator_loss', d_loss)\n",
    "      tf.contrib.summary.image('generator_image', tf.to_float(g_fake_images), max_images=9)\n",
    "\n",
    "      # get all discriminator variables (quantities to optimize)\n",
    "      d_variables = d_net.variables\n",
    "      # compute d(d_loss)/dx; where x is all discriminator variables\n",
    "      d_grads = d_tape.gradient(d_loss, d_variables)\n",
    "\n",
    "      # get all generator variables (quantities to optimize)\n",
    "      g_variables = g_net.variables\n",
    "      # compute d(g_loss)/dx; where x is all generator variables\n",
    "      g_grads = g_tape.gradient(g_loss, g_variables)\n",
    "\n",
    "      # update all variables\n",
    "      d_optimizer.apply_gradients(zip(d_grads, d_variables),\n",
    "                                  global_step=global_step)\n",
    "      g_optimizer.apply_gradients(zip(g_grads, g_variables),\n",
    "                                  global_step=global_step)\n",
    "\n",
    "  # output training status\n",
    "  counter = global_step.numpy()\n",
    "  if counter % 100==0:\n",
    "      print('training step {}: discriminator loss {}; generator loss {}'.format(counter, d_loss, g_loss))\n",
    "\n",
    "  # TRAINING PROCESS CONTROL FLOW\n",
    "  # every X steps, generate a batch of images\n",
    "  if counter % SAVE_IMG_PER_N_STEPS==0:\n",
    "      print('Current step:{}'.format(counter))\n",
    "      with tf.contrib.summary.always_record_summaries():\n",
    "          g_sample_images = g_net(eval_z, is_training=False)\n",
    "          tf.contrib.summary.image('fixed_latent_generator_image', \n",
    "                                   tf.to_float(g_sample_images), \n",
    "                                   max_images=BATCH_SIZE)\n",
    "      # save image as numpy array\n",
    "      np.save(img_sample_dir+'/{}.npy'.format(counter), g_sample_images.numpy()) \n",
    "          \n",
    "  if counter % SAVE_MODEL_PER_N_STEPS==0:\n",
    "      print('Saving model snapshot')\n",
    "      g_root.save(file_prefix=os.path.join(g_checkpoint_dir, \"model.ckpt\"))\n",
    "      d_root.save(file_prefix=os.path.join(d_checkpoint_dir, \"model.ckpt\"))\n",
    "      \n",
    "  # save generator model at end of training\n",
    "  if counter >= MAX_TRAINING_STEPS:\n",
    "      print('End of training; saving models')\n",
    "      g_root.save(file_prefix=os.path.join(g_checkpoint_dir, \"model.ckpt\"))\n",
    "      d_root.save(file_prefix=os.path.join(d_checkpoint_dir, \"model.ckpt\"))\n",
    "      sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uOeCQBMIpmPA"
   },
   "source": [
    "#### copy training log and model checkpoints to G-drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aaTDYoCdqPVf"
   },
   "outputs": [],
   "source": [
    "# copy results over to G-Drive\n",
    "!cp -r /content/SAGAN/trained_models/ /content/drive/'My Drive'/colab_notebooks/SAGAN/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xHJZMOpEHEJp"
   },
   "source": [
    "## 6 Generate Training Progression Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1uOcQ4YrcDAM"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCi-895vmWAo"
   },
   "outputs": [],
   "source": [
    "def scaleImg(dataset):\n",
    "    scaler = MinMaxScaler()\n",
    "    image = scaler.fit_transform(dataset.reshape(dataset.shape[0]*dataset.shape[1], dataset.shape[2]))\n",
    "    image = image.reshape(dataset.shape[0], dataset.shape[1], dataset.shape[2])\n",
    "    return image\n",
    "\n",
    "  \n",
    "def normalizeImg(img):\n",
    "    # img is expected to be 4D tensor with [batch, height, width, color]\n",
    "    # normalize pixel intensity in each image in batch\n",
    "    for i in range(img.shape[0]):\n",
    "        img[i,:,:,:] = scaleImg(img[i,:,:,:])\n",
    "        img[i,:,:,:] = img[i,:,:,:] / img[i,:,:,:].max() # extra normalization step\n",
    "    # convert to tensor\n",
    "    img = tf.constant(img)\n",
    "    # convert to image grid\n",
    "    img_grid = tf.contrib.gan.eval.image_grid(\n",
    "                    input_tensor=img,\n",
    "                    grid_shape=(8,8),\n",
    "                    image_shape=(64, 64),\n",
    "                    num_channels=3\n",
    "                )\n",
    "    return img_grid.numpy()\n",
    "  \n",
    "  \n",
    "def save_img_progress(file_prefix):\n",
    "    files = glob(file_prefix)\n",
    "    for f in files:\n",
    "        # load image array from file\n",
    "        img = np.load(f)\n",
    "        # normalize \n",
    "        img_grid = normalizeImg(img)\n",
    "        # save img grid to jpg\n",
    "        plt.imsave(f.replace('.npy','.jpg'), img_grid[0,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BT4dlj7XxLFK"
   },
   "outputs": [],
   "source": [
    "save_img_progress(file_prefix='/content/SAGAN/trained_models/VANGAN_4dlr_hingeloss_64px/img_samples/*0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7592,
     "status": "ok",
     "timestamp": 1541453427015,
     "user": {
      "displayName": "Jake Cheng",
      "photoUrl": "https://lh3.googleusercontent.com/-qpuoEuHtM6k/AAAAAAAAAAI/AAAAAAAAASM/Yutboujk5B4/s64/photo.jpg",
      "userId": "09150888735248367162"
     },
     "user_tz": 300
    },
    "id": "cEoqO33VyH5U",
    "outputId": "25d78aa9-dd5c-4f0e-8e77-b610b4147214"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/b4/cbb592964dfd71a9de6a5b08f882fd334fb99ae09ddc82081dbb2f718c81/imageio-2.4.1.tar.gz (3.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.3MB 10.0MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from imageio) (1.14.6)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from imageio) (4.0.0)\n",
      "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->imageio) (0.46)\n",
      "Building wheels for collected packages: imageio\n",
      "  Running setup.py bdist_wheel for imageio ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n",
      "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/e0/43/31/605de9372ceaf657f152d3d5e82f42cf265d81db8bbe63cde1\n",
      "Successfully built imageio\n",
      "Installing collected packages: imageio\n",
      "Successfully installed imageio-2.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "llho2MxOjOMs"
   },
   "outputs": [],
   "source": [
    "# make video\n",
    "import imageio\n",
    "frames = []\n",
    "\n",
    "image_folder = '/content/SAGAN/trained_models/{}/img_samples/'.format(MODEL_NAME)\n",
    "filenames = ['{}.jpg'.format(int(i)) for i in np.arange(1e3,1e5,1e3)]\n",
    "\n",
    "for f in filenames:\n",
    "    file = os.path.join(image_folder, f)\n",
    "    frames.append(imageio.imread(file))\n",
    "imageio.mimsave('/content/SAGAN/trained_models/{}/img_samples/progression.gif'.format(MODEL_NAME), frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fvyZ-K5QyXJp"
   },
   "outputs": [],
   "source": [
    "# copy results over to G-Drive\n",
    "!cp -r /content/SAGAN/trained_models/VANGAN_4dlr_hingeloss_64px/img_samples /content/drive/'My Drive'/colab_notebooks/SAGAN/trained_models/VANGAN_4dlr_hingeloss_64px/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "self_attention_gan_training.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
